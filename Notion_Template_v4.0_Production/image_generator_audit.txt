ESTATE PLANNING CONCIERGE v4.0 - IMAGE GENERATION SYSTEM AUDIT FILE
Generated: 2025-09-01 23:24:35
Total Files: 15
================================================================================

================================================================================
FILE: asset_generation/asset_generator.py
================================================================================

#!/usr/bin/env python3
import os
import sys
import json
import time
import logging
import logging.handlers
import asyncio
import replicate
import yaml
import requests
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from colorama import init, Fore, Back, Style
from tqdm import tqdm
from tqdm.asyncio import tqdm as atqdm
from git_operations import GitOperations
from sync_yaml_comprehensive import sync_with_yaml as comprehensive_sync
init(autoreset=True)
class ColoredFormatter(logging.Formatter):
    FORMATS = {
        logging.DEBUG: Fore.CYAN + "🔍 %(asctime)s [DEBUG] %(message)s" + Style.RESET_ALL,
        logging.INFO: Fore.GREEN + "✅ %(asctime)s [INFO] %(message)s" + Style.RESET_ALL,
        logging.WARNING: Fore.YELLOW + "⚠️  %(asctime)s [WARN] %(message)s" + Style.RESET_ALL,
        logging.ERROR: Fore.RED + "❌ %(asctime)s [ERROR] %(message)s" + Style.RESET_ALL,
        logging.CRITICAL: Fore.RED + Back.WHITE + "🚨 %(asctime)s [CRITICAL] %(message)s" + Style.RESET_ALL
    }
    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno, "%(asctime)s [%(levelname)s] %(message)s")
        formatter = logging.Formatter(log_fmt, datefmt='%H:%M:%S')
        return formatter.format(record)
class AssetGenerator:
    def __init__(self, config_path: str = "config.json"):
        self.start_time = time.time()
        self.total_cost = 0.0
        self.errors = []
        self.generated_assets = []
        self.generation_stats = {
            'icons_generated': 0,
            'covers_generated': 0,
            'textures_generated': 0,
            'regenerated_count': 0,
            'total_cost': 0.0,
            'generation_mode': 'sample'
        }
        self.config = self.load_config(config_path)
        self.setup_logging()
        self.setup_replicate()
        for dir_key in ['sample_directory', 'production_directory', 'backup_directory']:
            dir_path = Path(self.config['output'][dir_key])
            dir_path.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"✓ Directory ready: {dir_path}")
        self.logger.info("=" * 80)
        self.logger.info("ESTATE PLANNING CONCIERGE v4.0 - ASSET GENERATOR")
        self.logger.info("=" * 80)
        self.logger.info(f"Configuration loaded from: {config_path}")
        self.logger.info(f"Sample budget: ${self.config['budget']['sample_generation']['max_cost']:.2f}")
        self.logger.info(f"Production budget: ${self.config['budget']['mass_generation']['max_cost']:.2f}")
        self.logger.info(f"Review server port: {self.config['review']['port']}")
        self.logger.info("=" * 80)
    def load_config(self, config_path: str) -> Dict:
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        with open(config_file, 'r') as f:
            config = json.load(f)
        api_key = config['replicate']['api_key']
        if api_key.startswith('${') and api_key.endswith('}'):
            env_var = api_key[2:-1]
            config['replicate']['api_key'] = os.getenv(env_var, '')
        return config
    def setup_logging(self):
        self.logger = logging.getLogger('AssetGenerator')
        self.logger.setLevel(getattr(logging, self.config['logging']['level']))
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(ColoredFormatter())
        self.logger.addHandler(console_handler)
        log_file = Path(self.config['logging']['log_file'])
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=self.config['logging']['max_log_size'],
            backupCount=self.config['logging']['backup_count']
        )
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        self.logger.addHandler(file_handler)
    def setup_replicate(self):
        api_key = self.config['replicate']['api_key']
        if not api_key:
            raise ValueError("REPLICATE_API_KEY not found in environment or config")
        os.environ['REPLICATE_API_TOKEN'] = api_key
        self.logger.info("✓ Replicate API client initialized")
    def sync_with_yaml(self) -> Dict[str, List[Dict]]:
        self.logger.info("Using comprehensive YAML sync with fallback parsing...")
        try:
            pages_by_type = comprehensive_sync()
        except ValueError as e:
            self.logger.error("="*80)
            self.logger.error("CRITICAL ERROR: YAML PARSING FAILED")
            self.logger.error("="*80)
            self.logger.error(str(e))
            self.logger.error("="*80)
            self.logger.error("Script cannot continue without all YAML files.")
            self.logger.error("Please fix the YAML errors and try again.")
            self.logger.error("="*80)
            sys.exit(1)
        except Exception as e:
            self.logger.error(f"Unexpected error during YAML sync: {e}")
            sys.exit(1)
        self.logger.info("="*80)
        self.logger.info("COMPREHENSIVE YAML SYNC COMPLETE")
        self.logger.info("="*80)
        self.logger.info("Assets discovered:")
        for asset_type, items in pages_by_type.items():
            self.logger.info(f"  - {asset_type}: {len(items)}")
        self.logger.info(f"  - TOTAL ASSETS: {sum(len(v) for v in pages_by_type.values())}")
        self.logger.info("="*80)
        return pages_by_type
    def print_status(self, stage: str, message: str, level: str = "info"):
        elapsed = time.time() - self.start_time
        elapsed_str = f"{elapsed:.1f}s"
        cost_str = f"${self.total_cost:.3f}"
        status_line = f"[{elapsed_str}] [{cost_str}] {stage}: {message}"
        if level == "error":
            self.logger.error(status_line)
        elif level == "warning":
            self.logger.warning(status_line)
        else:
            self.logger.info(status_line)
    def confirm_action(self, message: str, cost: float = 0) -> bool:
        if cost > 0:
            message += f" (Estimated cost: ${cost:.2f})"
        print(f"\n{Fore.YELLOW}⚠️  {message}{Style.RESET_ALL}")
        response = input(f"{Fore.CYAN}Continue? (yes/no): {Style.RESET_ALL}")
        return response.lower() in ['yes', 'y']
    async def generate_asset(self, asset_type: str, prompt: str, index: int, total: int) -> Optional[Dict]:
        model_config = self.config['replicate']['models'][asset_type]
        model_id = model_config['model_id']
        cost = model_config['cost_per_image']
        if self.total_cost + cost > self.config['budget']['sample_generation']['max_cost']:
            self.print_status(
                "BUDGET",
                f"Would exceed sample budget (${self.total_cost + cost:.3f} > ${self.config['budget']['sample_generation']['max_cost']:.2f})",
                "warning"
            )
            return None
        self.print_status(
            f"{asset_type.upper()}",
            f"Generating {index}/{total}: {prompt[:50]}..."
        )
        try:
            await asyncio.sleep(1 / self.config['replicate']['rate_limit'])
            output = await asyncio.to_thread(
                replicate.run,
                model_id,
                input={"prompt": prompt}
            )
            self.total_cost += cost
            self.update_statistics(asset_type, count=1, cost=cost)
            file_ext = '.svg' if asset_type in ['icons', 'database_icons'] else '.png'
            filename = f"{asset_type}_{index:03d}{file_ext}"
            filepath = Path(self.config['output']['sample_directory']) / filename
            filepath.parent.mkdir(parents=True, exist_ok=True)
            if output:
                image_url = output[0] if isinstance(output, list) else output
                try:
                    response = await asyncio.to_thread(requests.get, image_url, stream=True, timeout=30)
                    response.raise_for_status()
                    with open(filepath, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    self.logger.debug(f"Downloaded {filename} from {image_url[:50]}...")
                except requests.exceptions.RequestException as e:
                    self.logger.error(f"Failed to download image: {str(e)}")
                    raise
            self.print_status(
                f"{asset_type.upper()}",
                f"✓ Generated {filename} (Cost: ${cost:.3f}, Total: ${self.total_cost:.3f})"
            )
            return {
                'type': asset_type,
                'filename': filename,
                'prompt': prompt,
                'prompt_editable': True,
                'cost': cost,
                'timestamp': datetime.now().isoformat(),
                'model_id': model_id,
                'index': index,
            }
        except Exception as e:
            self.print_status(
                f"{asset_type.upper()}",
                f"✗ Failed to generate {index}/{total}: {str(e)}",
                "error"
            )
            self.errors.append({
                'type': asset_type,
                'index': index,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            return None
    async def generate_samples(self):
        self.logger.info("\n" + "="*80)
        self.logger.info("STAGE 1: COMPREHENSIVE SAMPLE GENERATION")
        self.logger.info("="*80)
        pages_by_type = self.sync_with_yaml()
        samples = []
        sample_configs = []
        if pages_by_type['icons']:
            icon_samples = pages_by_type['icons'][:5]
            if icon_samples:
                sample_configs.append(('icons', icon_samples))
        if pages_by_type['covers']:
            cover_samples = pages_by_type['covers'][:5]
            if cover_samples:
                sample_configs.append(('covers', cover_samples))
        if pages_by_type['letter_headers']:
            letter_samples = pages_by_type['letter_headers'][:3]
            if letter_samples:
                sample_configs.append(('letter_headers', letter_samples))
        if pages_by_type['database_icons']:
            db_samples = pages_by_type['database_icons'][:5]
            if db_samples:
                sample_configs.append(('database_icons', db_samples))
        if pages_by_type['textures']:
            texture_samples = pages_by_type['textures'][:4]
            if texture_samples:
                sample_configs.append(('textures', texture_samples))
        asset_configs = sample_configs
        total_samples = sum(len(items) for _, items in asset_configs)
        with tqdm(total=total_samples, desc="Generating Samples", unit="asset") as pbar:
            for asset_type, page_items in asset_configs:
                count = len(page_items)
                self.print_status("SAMPLES", f"Starting {asset_type} generation ({count} items)")
                tasks = []
                for i, page_data in enumerate(page_items, 1):
                    prompt = page_data['prompt']
                    task = self.generate_asset_with_metadata(asset_type, prompt, i, count, page_data)
                    tasks.append(task)
                results = await asyncio.gather(*tasks)
                for result in results:
                    if result:
                        samples.append(result)
                        self.generated_assets.append(result)
                    pbar.update(1)
        manifest_path = Path(self.config['output']['sample_directory']) / "manifest.json"
        with open(manifest_path, 'w') as f:
            json.dump({
                'samples': samples,
                'total_cost': self.total_cost,
                'errors': self.errors,
                'timestamp': datetime.now().isoformat(),
                'editable_prompts': True,
                'prompt_file': 'prompts.json'
            }, f, indent=2)
        prompts_path = Path('prompts.json')
        if prompts_path.exists():
            with open(prompts_path, 'r') as f:
                prompts_data = json.load(f)
            for sample in samples:
                asset_type = sample['type']
                page_title = sample.get('metadata', {}).get('page_title', 'Unknown')
                if asset_type in prompts_data['prompts']:
                    prompts_data['prompts'][asset_type][page_title] = {
                        'original': sample['prompt'],
                        'current': sample['prompt'],
                        'generated_at': sample['timestamp'],
                        'filename': sample['filename'],
                        'metadata': sample.get('metadata', {})
                    }
            prompts_data['statistics']['total_prompts_generated'] = len(samples)
            prompts_data['statistics']['last_updated'] = datetime.now().isoformat()
            with open(prompts_path, 'w') as f:
                json.dump(prompts_data, f, indent=2)
        self.logger.info("\n" + "="*80)
        self.logger.info(f"SAMPLE GENERATION COMPLETE")
        self.logger.info(f"Generated: {len(samples)} samples")
        self.logger.info(f"Errors: {len(self.errors)}")
        self.logger.info(f"Total Cost: ${self.total_cost:.3f}")
        self.logger.info(f"Time Elapsed: {time.time() - self.start_time:.1f}s")
        self.logger.info("="*80)
        return samples
    async def generate_asset_with_metadata(self, asset_type: str, prompt: str, index: int, total: int, page_data: Dict) -> Optional[Dict]:
        result = await self.generate_asset(asset_type, prompt, index, total)
        if result:
            result['metadata'] = {
                'page_title': page_data.get('title', 'Unknown'),
                'page_description': page_data.get('description', ''),
                'page_slug': page_data.get('slug', ''),
                'page_role': page_data.get('role', ''),
                'original_prompt': prompt,
                'editable': True,
                'regeneration_count': 0,
                'prompt_history': [prompt],
            }
            result['regeneration'] = {
                'can_regenerate': True,
                'asset_type': asset_type,
                'model_config': self.config['replicate']['models'][asset_type],
                'page_index': index - 1,
            }
        return result
    async def run_sample_generation(self):
        try:
            samples = await self.generate_samples()
            if not samples:
                self.logger.error("No samples generated. Exiting.")
                return False
            self.logger.info("\n" + "="*80)
            self.logger.info("LAUNCHING REVIEW SERVER")
            self.logger.info("="*80)
            from review_server import ReviewServer
            server = ReviewServer(
                port=self.config['review']['port'],
                directory=self.config['output']['sample_directory'],
                auto_open=self.config['review']['auto_open']
            )
            self.logger.info(f"Starting review server on port {self.config['review']['port']}...")
            self.logger.info(f"Review URL: http://localhost:{self.config['review']['port']}")
            if self.config['review']['auto_open']:
                self.logger.info("Browser will open automatically...")
            approval_file = Path(self.config['review']['approval_file'])
            self.logger.info(f"\nWaiting for approval file: {approval_file}")
            self.logger.info("To approve samples, create APPROVED.txt in the current directory")
            approved = await server.wait_for_approval(approval_file)
            if approved:
                self.logger.info(Fore.GREEN + "\n✅ SAMPLES APPROVED - Proceeding to mass generation" + Style.RESET_ALL)
                return True
            else:
                self.logger.warning(Fore.YELLOW + "\n⚠️ SAMPLES REJECTED - Exiting" + Style.RESET_ALL)
                return False
        except Exception as e:
            self.logger.error(f"Sample generation failed: {str(e)}")
            return False
    async def generate_mass_production(self):
        self.logger.info("\n" + "="*80)
        self.logger.info("STAGE 2: MASS PRODUCTION GENERATION")
        self.logger.info("="*80)
        pages_by_type = self.sync_with_yaml()
        total_assets = sum(len(items) for items in pages_by_type.values())
        estimated_cost = 0.0
        for asset_type, items in pages_by_type.items():
            if asset_type in self.config['replicate']['models']:
                cost_per_item = self.config['replicate']['models'][asset_type]['cost_per_image']
                estimated_cost += len(items) * cost_per_item
        self.logger.info(f"Total assets to generate: {total_assets}")
        self.logger.info(f"Estimated cost: ${estimated_cost:.2f}")
        self.logger.info(f"Budget limit: ${self.config['budget']['mass_generation']['max_cost']:.2f}")
        if not self.confirm_action(f"Mass generation will generate {total_assets} assets", estimated_cost):
            self.logger.warning("Mass generation cancelled by user")
            return False
        self.total_cost = 0.0
        self.generated_assets = []
        production_budget = self.config['budget']['mass_generation']['max_cost']
        original_sample_dir = self.config['output']['sample_directory']
        self.config['output']['sample_directory'] = self.config['output']['production_directory']
        try:
            with tqdm(total=total_assets, desc="Mass Production", unit="asset") as pbar:
                for asset_type, items in pages_by_type.items():
                    if asset_type not in self.config['replicate']['models']:
                        self.logger.warning(f"Skipping {asset_type}: no model configured")
                        pbar.update(len(items))
                        continue
                    self.print_status("PRODUCTION", f"Generating {len(items)} {asset_type}")
                    for i, item in enumerate(items, 1):
                        model_config = self.config['replicate']['models'][asset_type]
                        cost = model_config['cost_per_image']
                        if self.total_cost + cost > production_budget:
                            self.logger.error(f"Budget limit would be exceeded: ${self.total_cost + cost:.2f} > ${production_budget:.2f}")
                            self.logger.warning(f"Stopping after {len(self.generated_assets)} assets")
                            return False
                        try:
                            result = await self.generate_asset(asset_type, item['prompt'], i, len(items))
                            if result:
                                result['metadata'] = {
                                    'title': item.get('title', 'Unknown'),
                                    'slug': item.get('slug', ''),
                                    'category': item.get('category', ''),
                                    'production': True
                                }
                                self.generated_assets.append(result)
                        except Exception as e:
                            self.logger.error(f"Failed to generate {asset_type}
                            self.errors.append({
                                'type': asset_type,
                                'index': i,
                                'title': item.get('title', 'Unknown'),
                                'error': str(e)
                            })
                        pbar.update(1)
                        if len(self.generated_assets) % 10 == 0:
                            self.print_status(
                                "PROGRESS",
                                f"Generated {len(self.generated_assets)}/{total_assets} assets"
                            )
            manifest_path = Path(self.config['output']['production_directory']) / "manifest.json"
            with open(manifest_path, 'w') as f:
                json.dump({
                    'assets': self.generated_assets,
                    'total_cost': self.total_cost,
                    'errors': self.errors,
                    'timestamp': datetime.now().isoformat(),
                    'production': True,
                    'total_generated': len(self.generated_assets),
                    'total_expected': total_assets
                }, f, indent=2)
            self.logger.info("\n" + "="*80)
            self.logger.info(f"✅ MASS PRODUCTION COMPLETE")
            self.logger.info(f"Generated: {len(self.generated_assets)} assets")
            self.logger.info(f"Errors: {len(self.errors)}")
            self.logger.info(f"Total Cost: ${self.total_cost:.2f}")
            self.logger.info(f"Time Elapsed: {time.time() - self.start_time:.1f}s")
            self.logger.info("="*80)
            return True
        finally:
            self.config['output']['sample_directory'] = original_sample_dir
    async def regenerate_specific_asset(self, asset_data: Dict) -> Optional[Dict]:
        asset_type = asset_data['type']
        prompt = asset_data['prompt']
        filename = asset_data['filename']
        page_title = asset_data.get('page_title', 'Unknown')
        self.logger.info(f"Regenerating {filename} with new prompt...")
        model_config = self.config['replicate']['models'][asset_type]
        model_id = model_config['model_id']
        cost = model_config['cost_per_image']
        try:
            await asyncio.sleep(1 / self.config['replicate']['rate_limit'])
            output = await asyncio.to_thread(
                replicate.run,
                model_id,
                input={"prompt": prompt}
            )
            self.total_cost += cost
            self.update_statistics(asset_type, count=1, cost=cost)
            self.generation_stats['regenerated_count'] += 1
            filepath = Path(self.config['output']['sample_directory']) / filename
            if output:
                image_url = output[0] if isinstance(output, list) else output
                response = await asyncio.to_thread(requests.get, image_url, stream=True, timeout=30)
                response.raise_for_status()
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
            self.logger.info(f"✅ Regenerated {filename} successfully")
            prompts_path = Path('prompts.json')
            if prompts_path.exists():
                with open(prompts_path, 'r') as f:
                    prompts_data = json.load(f)
                prompts_data['statistics']['total_regenerations'] += 1
                prompts_data['statistics']['last_updated'] = datetime.now().isoformat()
                with open(prompts_path, 'w') as f:
                    json.dump(prompts_data, f, indent=2)
            return {
                'filename': filename,
                'type': asset_type,
                'prompt': prompt,
                'cost': cost,
                'regenerated_at': datetime.now().isoformat(),
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"Failed to regenerate {filename}: {str(e)}")
            return {
                'filename': filename,
                'error': str(e),
                'status': 'failed'
            }
    async def process_regeneration_queue(self):
        prompts_path = Path('prompts.json')
        if not prompts_path.exists():
            self.logger.warning("No prompts.json found")
            return
        with open(prompts_path, 'r') as f:
            prompts_data = json.load(f)
        queue = prompts_data.get('regeneration_queue', [])
        if not queue:
            self.logger.info("No assets queued for regeneration")
            return
        self.logger.info(f"Processing {len(queue)} regeneration requests...")
        results = []
        for item in queue:
            result = await self.regenerate_specific_asset(item)
            results.append(result)
        prompts_data['regeneration_queue'] = []
        with open(prompts_path, 'w') as f:
            json.dump(prompts_data, f, indent=2)
        successful = sum(1 for r in results if r.get('status') == 'success')
        failed = sum(1 for r in results if r.get('status') == 'failed')
        self.logger.info(f"Regeneration complete: {successful} successful, {failed} failed")
        return results
    def print_final_summary(self):
        elapsed = time.time() - self.start_time
        print("\n" + "="*80)
        print(Fore.CYAN + "FINAL SUMMARY" + Style.RESET_ALL)
        print("="*80)
        print(f"\n📊 STATISTICS:")
        print(f"  • Total Assets Generated: {len(self.generated_assets)}")
        print(f"  • Total Cost: ${self.total_cost:.3f}")
        print(f"  • Total Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)")
        print(f"  • Average Cost per Asset: ${self.total_cost/max(len(self.generated_assets), 1):.3f}")
        print(f"  • Errors Encountered: {len(self.errors)}")
        if self.errors:
            print(f"\n⚠️ ERRORS:")
            for error in self.errors[:5]:
                print(f"  • {error['type']}
        print(f"\n📁 OUTPUT LOCATIONS:")
        print(f"  • Samples: {self.config['output']['sample_directory']}")
        print(f"  • Logs: {self.config['logging']['log_file']}")
        print("\n" + "="*80)
    def update_statistics(self, asset_type: str, count: int = 1, cost: float = 0.0):
        if asset_type == 'icons':
            self.generation_stats['icons_generated'] += count
        elif asset_type == 'covers':
            self.generation_stats['covers_generated'] += count
        elif asset_type == 'textures':
            self.generation_stats['textures_generated'] += count
        self.generation_stats['total_cost'] += cost
        self.total_cost += cost
    async def commit_assets_to_git(self, mode: str = "sample", dry_run: bool = False):
        try:
            self.logger.info("Preparing to commit assets to Git...")
            git_ops = GitOperations(repo_path=Path.cwd().parent)
            self.generation_stats['generation_mode'] = mode
            self.generation_stats['total_cost'] = self.total_cost
            success = git_ops.auto_commit_assets(
                stats=self.generation_stats,
                mode=mode,
                dry_run=dry_run
            )
            if success:
                self.logger.info("✅ Assets successfully committed to Git")
            else:
                self.logger.warning("⚠️ Git commit failed - assets saved locally but not committed")
            return success
        except Exception as e:
            self.logger.error(f"Error during Git commit: {str(e)}")
            return False
async def main():
    import argparse
    parser = argparse.ArgumentParser(description="Estate Planning Asset Generator")
    parser.add_argument('--regenerate', action='store_true',
                       help='Process regeneration queue from prompts.json')
    parser.add_argument('--skip-review', action='store_true',
                       help='Skip review server and approval process')
    parser.add_argument('--mass-production', action='store_true',
                       help='Run mass production immediately (requires prior approval)')
    parser.add_argument('--edit-prompts', action='store_true',
                       help='Launch prompt editor interface only')
    parser.add_argument('--no-commit', action='store_true',
                       help='Skip automatic Git commit after generation')
    parser.add_argument('--dry-run', action='store_true',
                       help='Preview Git operations without executing them')
    args = parser.parse_args()
    generator = AssetGenerator()
    try:
        if args.regenerate:
            generator.logger.info("Processing regeneration queue...")
            await generator.process_regeneration_queue()
        elif args.edit_prompts:
            generator.logger.info("Launching prompt editor interface...")
            from review_server import ReviewServer
            server = ReviewServer()
            server.start()
            print("\n📝 Prompt editor running. Press Enter to stop...")
            input()
            server.stop()
        elif args.mass_production:
            if not Path("APPROVED.txt").exists():
                generator.logger.error("Mass production requires approval. Run sample generation first.")
                return
            generator.logger.info("Starting mass production...")
            await generator.generate_mass_production()
            if not args.no_commit:
                await generator.commit_assets_to_git(mode="production", dry_run=args.dry_run)
        else:
            approved = await generator.run_sample_generation()
            if approved and not args.skip_review:
                await generator.process_regeneration_queue()
                await generator.generate_mass_production()
                if not args.no_commit:
                    await generator.commit_assets_to_git(mode="production", dry_run=args.dry_run)
            elif args.skip_review:
                if not args.no_commit:
                    await generator.commit_assets_to_git(mode="sample", dry_run=args.dry_run)
    except KeyboardInterrupt:
        generator.logger.warning("\n⚠️ Generation interrupted by user")
    except Exception as e:
        generator.logger.error(f"Fatal error: {str(e)}")
    finally:
        generator.print_final_summary()
if __name__ == "__main__":
    asyncio.run(main())

================================================================================
FILE: asset_generation/openrouter_orchestrator.py
================================================================================

#!/usr/bin/env python3
import os
import json
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from pathlib import Path
import re
@dataclass
class StructuredPrompt:
    system_message: str
    temperature: float
    role: str
    prompt: str
    raw_response: str
@dataclass
class PromptVariant:
    model: str
    structured_prompt: StructuredPrompt
    style_elements: List[str]
    emotional_markers: List[str]
    luxury_indicators: List[str]
    confidence: float
    reasoning: str
    timestamp: str
@dataclass
class PromptCompetition:
    page_title: str
    page_category: str
    asset_type: str
    variants: List[PromptVariant]
    winner: Optional[PromptVariant] = None
    human_selected: Optional[PromptVariant] = None
    scores: Optional[Dict[str, float]] = None
class OpenRouterOrchestrator:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')
        if not self.api_key:
            raise ValueError("OpenRouter API key is required")
        self.models = {
            'claude': {
                'id': 'anthropic/claude-3-opus-20240229',
                'perspective': 'emotional_depth',
                'strengths': ['empathy', 'nuance', 'human_connection']
            },
            'gpt4': {
                'id': 'openai/gpt-4-turbo-preview',
                'perspective': 'creative_luxury',
                'strengths': ['creativity', 'luxury', 'sophistication']
            },
            'gemini': {
                'id': 'google/gemini-pro',
                'perspective': 'technical_precision',
                'strengths': ['precision', 'consistency', 'structure']
            }
        }
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.logger = self._setup_logger()
        self.master_prompt = self._load_master_prompt()
        self.logs_dir = Path("logs/llm_generations")
        self.logs_dir.mkdir(parents=True, exist_ok=True)
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger('OpenRouterOrchestrator')
        logger.setLevel(logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        fh = logging.FileHandler('openrouter_orchestration.log')
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        return logger
    def _load_master_prompt(self) -> str:
        master_prompt_path = Path("meta_prompts/master_prompt.txt")
        try:
            with open(master_prompt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            self.logger.info(f"Loaded master prompt from {master_prompt_path}")
            return content
        except FileNotFoundError:
            self.logger.error(f"Master prompt file not found: {master_prompt_path}")
            raise ValueError(f"Master prompt file not found: {master_prompt_path}")
        except Exception as e:
            self.logger.error(f"Failed to load master prompt: {e}")
            raise
    def _parse_structured_response(self, raw_response: str) -> StructuredPrompt:
        try:
            system_match = re.search(r'SYSTEM:\s*(.*?)(?=\n\nTEMPERATURE:|$)', raw_response, re.DOTALL | re.IGNORECASE)
            temp_match = re.search(r'TEMPERATURE:\s*([\d.]+)', raw_response, re.IGNORECASE)
            role_match = re.search(r'ROLE:\s*(.*?)(?=\n\nPROMPT:|$)', raw_response, re.DOTALL | re.IGNORECASE)
            prompt_match = re.search(r'PROMPT:\s*(.*?)(?=\n\n---|\n\n[A-Z]+:|\Z)', raw_response, re.DOTALL | re.IGNORECASE)
            system_message = system_match.group(1).strip() if system_match else "You are a luxury brand designer specializing in estate planning visuals."
            temperature = float(temp_match.group(1)) if temp_match else 0.5
            role = role_match.group(1).strip() if role_match else "luxury brand designer"
            prompt = prompt_match.group(1).strip() if prompt_match else raw_response.strip()
            temperature = max(0.1, min(1.0, temperature))
            return StructuredPrompt(
                system_message=system_message,
                temperature=temperature,
                role=role,
                prompt=prompt,
                raw_response=raw_response
            )
        except Exception as e:
            self.logger.warning(f"Failed to parse structured response, using fallback: {e}")
            return StructuredPrompt(
                system_message="You are a luxury brand designer specializing in estate planning visuals.",
                temperature=0.5,
                role="luxury brand designer",
                prompt=raw_response.strip(),
                raw_response=raw_response
            )
    def _log_llm_interaction(self, model: str, context_data: Dict[str, Any], response: StructuredPrompt, timestamp: str):
        log_entry = {
            'timestamp': timestamp,
            'model': model,
            'context_data': context_data,
            'structured_response': {
                'system_message': response.system_message,
                'temperature': response.temperature,
                'role': response.role,
                'prompt': response.prompt,
                'raw_response': response.raw_response
            }
        }
        log_filename = f"llm_generation_{model}_{timestamp.replace(':', '-').replace('.', '-')}.json"
        log_path = self.logs_dir / log_filename
        try:
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(log_entry, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Logged LLM interaction to {log_path}")
        except Exception as e:
            self.logger.error(f"Failed to log LLM interaction: {e}")
        master_log = self.logs_dir / "master_llm_log.jsonl"
        try:
            with open(master_log, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            self.logger.error(f"Failed to append to master log: {e}")
    async def _call_openrouter_with_master_prompt(self, model_id: str, context_data: Dict[str, Any]) -> Dict[str, Any]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://estate-planning-concierge.com",
            "X-Title": "Estate Planning Concierge v4.0"
        }
        context_section = f
        full_prompt = self.master_prompt + context_section
        payload = {
            "model": model_id,
            "messages": [
                {
                    "role": "user",
                    "content": full_prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 2000
        }
        timestamp = datetime.now().isoformat()
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(self.base_url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        raw_response = result['choices'][0]['message']['content']
                        structured_prompt = self._parse_structured_response(raw_response)
                        self._log_llm_interaction(model_id, context_data, structured_prompt, timestamp)
                        return {
                            'success': True,
                            'structured_prompt': structured_prompt,
                            'model': model_id,
                            'usage': result.get('usage', {}),
                            'timestamp': timestamp
                        }
                    else:
                        error_text = await response.text()
                        self.logger.error(f"OpenRouter API error: {response.status} - {error_text}")
                        return {
                            'success': False,
                            'error': f"API error: {response.status}",
                            'model': model_id
                        }
            except Exception as e:
                self.logger.error(f"OpenRouter request failed: {str(e)}")
                return {
                    'success': False,
                    'error': str(e),
                    'model': model_id
                }
    def _prepare_context_data(self, page_info: Dict[str, Any], model_config: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'title': page_info['title'],
            'category': page_info['category'],
            'asset_type': page_info['asset_type'],
            'section': page_info.get('section', 'general'),
            'tier': page_info.get('tier', 'standard'),
            'emotional_context': page_info.get('emotional_context', 'DIGNIFIED_PLANNING'),
            'model_perspective': model_config['perspective'],
            'model_strengths': model_config['strengths']
        }
    async def generate_competitive_prompts(self, page_info: Dict[str, Any]) -> PromptCompetition:
        self.logger.info(f"Generating competitive prompts for: {page_info['title']}")
        tasks = []
        for model_name, model_config in self.models.items():
            context_data = self._prepare_context_data(page_info, model_config)
            task = self._call_openrouter_with_master_prompt(model_config['id'], context_data)
            tasks.append((model_name, task))
        results = []
        for model_name, task in tasks:
            result = await task
            if result['success']:
                structured_prompt = result['structured_prompt']
                style_elements = self._extract_style_elements(structured_prompt.prompt)
                emotional_markers = self._extract_emotional_markers(structured_prompt.prompt)
                luxury_indicators = self._extract_luxury_indicators(structured_prompt.prompt)
                variant = PromptVariant(
                    model=model_name,
                    structured_prompt=structured_prompt,
                    style_elements=style_elements,
                    emotional_markers=emotional_markers,
                    luxury_indicators=luxury_indicators,
                    confidence=0.8,
                    reasoning=f"Generated using master prompt with {model_config['perspective']} perspective",
                    timestamp=result['timestamp']
                )
                results.append(variant)
                self.logger.info(f"Successfully generated structured prompt from {model_name}")
            else:
                self.logger.error(f"Failed to get response from {model_name}: {result.get('error')}")
        competition = PromptCompetition(
            page_title=page_info['title'],
            page_category=page_info['category'],
            asset_type=page_info['asset_type'],
            variants=results
        )
        return competition
    def _extract_style_elements(self, prompt: str) -> List[str]:
        style_keywords = ['luxury', 'premium', 'elegant', 'sophisticated', 'warm', 'mahogany',
                         'leather', 'gold', 'marble', 'crystal', 'silk', 'velvet', 'bronze']
        found_elements = [keyword for keyword in style_keywords if keyword.lower() in prompt.lower()]
        return found_elements[:5]
    def _extract_emotional_markers(self, prompt: str) -> List[str]:
        emotional_keywords = ['compassionate', 'warm', 'gentle', 'caring', 'family', 'legacy',
                             'heritage', 'memories', 'comfort', 'trust', 'dignity', 'respect']
        found_markers = [keyword for keyword in emotional_keywords if keyword.lower() in prompt.lower()]
        return found_markers[:5]
    def _extract_luxury_indicators(self, prompt: str) -> List[str]:
        luxury_keywords = ['private', 'exclusive', 'bespoke', 'handcrafted', 'artisanal',
                          'executive', 'prestige', 'estate', 'mansion', 'penthouse', 'limousine']
        found_indicators = [keyword for keyword in luxury_keywords if keyword.lower() in prompt.lower()]
        return found_indicators[:5]
    async def score_prompts(self, competition: PromptCompetition) -> PromptCompetition:
        self.logger.info(f"Scoring prompts for: {competition.page_title}")
        scoring_prompt = f
        for i, variant in enumerate(competition.variants, 1):
            scoring_prompt += f
        scoring_prompt +=
        result = await self._call_openrouter(self.models['gpt4']['id'], scoring_prompt, temperature=0.3)
        if result['success']:
            try:
                scores = json.loads(result['content'])
                competition.scores = scores['scores']
                if scores.get('recommended'):
                    winner_idx = scores['recommended'] - 1
                    if 0 <= winner_idx < len(competition.variants):
                        competition.winner = competition.variants[winner_idx]
                self.logger.info(f"Successfully scored prompts, winner: {competition.winner.model if competition.winner else 'None'}")
            except json.JSONDecodeError as e:
                self.logger.error(f"Failed to parse scoring response: {e}")
        return competition
    async def orchestrate_competition(self, pages: List[Dict[str, Any]]) -> List[PromptCompetition]:
        competitions = []
        for page in pages:
            self.logger.info(f"Starting competition for: {page['title']}")
            competition = await self.generate_competitive_prompts(page)
            competition = await self.score_prompts(competition)
            competitions.append(competition)
            await asyncio.sleep(1)
        return competitions
    def save_competition_results(self, competitions: List[PromptCompetition], output_file: str = "prompt_competitions.json"):
        output_path = Path(output_file)
        data = []
        for comp in competitions:
            variants_data = []
            for v in comp.variants:
                variant_dict = {
                    'model': v.model,
                    'structured_prompt': {
                        'system_message': v.structured_prompt.system_message,
                        'temperature': v.structured_prompt.temperature,
                        'role': v.structured_prompt.role,
                        'prompt': v.structured_prompt.prompt,
                        'raw_response': v.structured_prompt.raw_response
                    },
                    'style_elements': v.style_elements,
                    'emotional_markers': v.emotional_markers,
                    'luxury_indicators': v.luxury_indicators,
                    'confidence': v.confidence,
                    'reasoning': v.reasoning,
                    'timestamp': v.timestamp
                }
                variants_data.append(variant_dict)
            comp_dict = {
                'page_title': comp.page_title,
                'page_category': comp.page_category,
                'asset_type': comp.asset_type,
                'variants': variants_data,
                'winner': {
                    'model': comp.winner.model,
                    'structured_prompt': {
                        'system_message': comp.winner.structured_prompt.system_message,
                        'temperature': comp.winner.structured_prompt.temperature,
                        'role': comp.winner.structured_prompt.role,
                        'prompt': comp.winner.structured_prompt.prompt,
                        'raw_response': comp.winner.structured_prompt.raw_response
                    } if comp.winner else None
                } if comp.winner else None,
                'human_selected': {
                    'model': comp.human_selected.model,
                    'structured_prompt': {
                        'system_message': comp.human_selected.structured_prompt.system_message,
                        'temperature': comp.human_selected.structured_prompt.temperature,
                        'role': comp.human_selected.structured_prompt.role,
                        'prompt': comp.human_selected.structured_prompt.prompt,
                        'raw_response': comp.human_selected.structured_prompt.raw_response
                    } if comp.human_selected else None
                } if comp.human_selected else None,
                'scores': comp.scores
            }
            data.append(comp_dict)
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)
        self.logger.info(f"Saved {len(competitions)} competition results to {output_path}")
        return output_path
async def test_orchestrator():
    orchestrator = OpenRouterOrchestrator()
    test_pages = [
        {
            'title': 'Executor Hub',
            'category': 'executor',
            'asset_type': 'icon',
            'section': 'executor',
            'tier': 'hub'
        },
        {
            'title': 'Family Messages',
            'category': 'family',
            'asset_type': 'cover',
            'section': 'family',
            'tier': 'section'
        }
    ]
    competitions = await orchestrator.orchestrate_competition(test_pages)
    output_file = orchestrator.save_competition_results(competitions)
    print(f"Competition complete! Results saved to: {output_file}")
    for comp in competitions:
        print(f"\n{comp.page_title}:")
        print(f"  Variants generated: {len(comp.variants)}")
        for variant in comp.variants:
            print(f"    {variant.model}: {variant.structured_prompt.prompt[:100]}...")
            print(f"      System: {variant.structured_prompt.system_message[:50]}...")
            print(f"      Temperature: {variant.structured_prompt.temperature}")
            print(f"      Role: {variant.structured_prompt.role}")
        if comp.winner:
            print(f"  Winner: {comp.winner.model} (confidence: {comp.winner.confidence:.2f})")
        print(f"  Scores: {comp.scores}")
if __name__ == "__main__":
    asyncio.run(test_orchestrator())

================================================================================
FILE: asset_generation/sync_yaml_comprehensive.py
================================================================================

#!/usr/bin/env python3
import yaml
from pathlib import Path
from typing import Dict, List, Set
import logging
from datetime import datetime
from prompt_templates import PromptTemplateManager, PageTier, AssetType
from visual_hierarchy import VisualHierarchyManager, VisualTier
from emotional_elements import EmotionalElementsManager, EmotionalContext, ComfortLevel
class YAMLSyncComprehensive:
    def __init__(self, yaml_dir: str = "../split_yaml"):
        self.yaml_dir = Path(yaml_dir)
        self.logger = logging.getLogger('YAMLSync')
        self.prompt_manager = PromptTemplateManager()
        self.hierarchy_manager = VisualHierarchyManager()
        self.emotional_manager = EmotionalElementsManager()
        self.logger.info("Initialized ultra-premium prompt generation system")
    def discover_pages(self) -> List[Dict]:
        return self._discover_all_pages()
    def sync_with_yaml(self) -> Dict[str, List[Dict]]:
        all_pages = self._discover_all_pages()
        pages_by_type = {
            'icons': [],
            'covers': [],
            'textures': [],
            'letter_headers': [],
            'database_icons': []
        }
        for page_info in all_pages:
            title = page_info['title']
            category = page_info.get('category', 'general')
            page_type = page_info.get('type', 'page')
            slug = title.lower().replace(' ', '-').replace('–', '-')
            slug = ''.join(c for c in slug if c.isalnum() or c == '-')
            visual_tier = self.hierarchy_manager.determine_visual_tier(title, category, 'icon')
            emotional_context = self._determine_emotional_context(title, category)
            if page_type != 'letter':
                enhanced_icon_prompt = self._generate_enhanced_icon_prompt(title, category, page_type, visual_tier, emotional_context)
                pages_by_type['icons'].append({
                    'title': title,
                    'slug': slug,
                    'category': category,
                    'visual_tier': visual_tier.value,
                    'emotional_context': emotional_context.value,
                    'prompt': enhanced_icon_prompt
                })
            if page_type != 'letter':
                enhanced_cover_prompt = self._generate_enhanced_cover_prompt(title, category, page_type, visual_tier, emotional_context)
                pages_by_type['covers'].append({
                    'title': title,
                    'slug': slug,
                    'category': category,
                    'visual_tier': visual_tier.value,
                    'emotional_context': emotional_context.value,
                    'prompt': enhanced_cover_prompt
                })
            elif page_type == 'letter':
                enhanced_header_prompt = self._generate_enhanced_letter_header_prompt(title, emotional_context)
                pages_by_type['letter_headers'].append({
                    'title': title,
                    'slug': slug,
                    'emotional_context': emotional_context.value,
                    'prompt': enhanced_header_prompt
                })
        pages_by_type['textures'] = self._get_enhanced_textures()
        pages_by_type['database_icons'] = self._get_enhanced_database_icons()
        total_assets = sum(len(v) for v in pages_by_type.values())
        self.logger.info(f"\n🏆 ULTRA-PREMIUM ASSET DISCOVERY COMPLETE 🏆")
        self.logger.info(f"Discovered {len(all_pages)} unique pages with luxury enhancements")
        self.logger.info(f"  Icons: {len(pages_by_type['icons'])} (with emotional intelligence)")
        self.logger.info(f"  Covers: {len(pages_by_type['covers'])} (with 5-tier hierarchy)")
        self.logger.info(f"  Letter headers: {len(pages_by_type['letter_headers'])} (with premium aesthetics)")
        self.logger.info(f"  Database icons: {len(pages_by_type['database_icons'])} (with enhanced styling)")
        self.logger.info(f"  Textures: {len(pages_by_type['textures'])} (with luxury materials)")
        self.logger.info(f"\n📊 Total ultra-premium assets: {total_assets}")
        self.logger.info(f"💎 All prompts enhanced with emotional intelligence & luxury aesthetics")
        tier_counts = {}
        for asset_list in [pages_by_type['icons'], pages_by_type['covers']]:
            for asset in asset_list:
                tier = asset.get('visual_tier', 'unknown')
                tier_counts[tier] = tier_counts.get(tier, 0) + 1
        if tier_counts:
            self.logger.info(f"\n🎯 Visual Tier Distribution:")
            for tier, count in sorted(tier_counts.items()):
                self.logger.info(f"  {tier}: {count} assets")
        return pages_by_type
    def _discover_all_pages(self) -> List[Dict]:
        all_pages = []
        seen_titles = set()
        yaml_files = sorted(self.yaml_dir.glob("*.yaml"))
        expected_count = len(yaml_files)
        processed_count = 0
        failed_files = []
        self.logger.info(f"Found {expected_count} YAML files to process")
        for yaml_file in yaml_files:
            try:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                try:
                    data = yaml.safe_load(content)
                    if data:
                        self._extract_from_structure(data, all_pages, seen_titles)
                        processed_count += 1
                except yaml.YAMLError:
                    self._extract_from_lines(content, all_pages, seen_titles)
                    processed_count += 1
                    self.logger.warning(f"Used fallback parsing for {yaml_file.name}")
            except Exception as e:
                self.logger.error(f"Failed to process {yaml_file.name}: {e}")
                failed_files.append(yaml_file.name)
        if failed_files:
            self.logger.error(f"Failed to process {len(failed_files)} files: {failed_files}")
            error_msg = f"Failed to process all YAML files. {len(failed_files)} failed out of {expected_count}:\n"
            for file in failed_files:
                error_msg += f"  - {file}\n"
            raise ValueError(error_msg)
        if processed_count != expected_count:
            error_msg = f"YAML file count mismatch! Expected {expected_count}, processed {processed_count}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        self.logger.info(f"✓ Successfully processed all {processed_count}/{expected_count} YAML files")
        return all_pages
    def _extract_from_structure(self, data: dict, all_pages: list, seen_titles: set):
        if not isinstance(data, dict):
            return
        if 'pages' in data and isinstance(data['pages'], list):
            for page in data['pages']:
                if isinstance(page, dict) and 'title' in page:
                    title = page['title']
                    if title not in seen_titles:
                        seen_titles.add(title)
                        all_pages.append({
                            'title': title,
                            'category': page.get('role', 'general'),
                            'type': 'page'
                        })
        if 'letters' in data and isinstance(data['letters'], list):
            for letter in data['letters']:
                if isinstance(letter, dict):
                    title = letter.get('Title') or letter.get('title')
                    if title and title not in seen_titles:
                        seen_titles.add(title)
                        all_pages.append({
                            'title': title,
                            'category': 'letters',
                            'type': 'letter'
                        })
        if 'acceptance' in data and isinstance(data['acceptance'], dict):
            if 'rows' in data['acceptance']:
                for row in data['acceptance']['rows']:
                    if isinstance(row, dict) and 'Page' in row:
                        title = row['Page']
                        if title not in seen_titles:
                            seen_titles.add(title)
                            all_pages.append({
                                'title': title,
                                'category': row.get('Section', 'general'),
                                'type': 'page'
                            })
        if 'admin_page' in data and isinstance(data['admin_page'], dict):
            if 'title' in data['admin_page']:
                title = data['admin_page']['title']
                if title not in seen_titles:
                    seen_titles.add(title)
                    all_pages.append({
                        'title': title,
                        'category': 'admin',
                        'type': 'page'
                    })
    def _extract_from_lines(self, content: str, all_pages: list, seen_titles: set):
        lines = content.split('\n')
        for line in lines:
            if line.strip().startswith('#'):
                continue
            if line.strip().startswith('- title:'):
                title = line.split(':', 1)[1].strip().strip('"').strip("'")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    all_pages.append({
                        'title': title,
                        'category': 'general',
                        'type': 'page'
                    })
            elif line.strip().startswith('- Title:'):
                title = line.split(':', 1)[1].strip().strip('"').strip("'")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    all_pages.append({
                        'title': title,
                        'category': 'letters',
                        'type': 'letter'
                    })
            elif 'Page:' in line and not line.strip().startswith('#'):
                parts = line.split('Page:', 1)
                if len(parts) > 1:
                    title = parts[1].strip().strip('"').strip("'")
                    if title and title != 'Page' and title not in seen_titles:
                        seen_titles.add(title)
                        all_pages.append({
                            'title': title,
                            'category': 'general',
                            'type': 'page'
                        })
            elif (line.startswith('  title:') or line.startswith('    title:')) and '{}' not in line:
                title = line.split(':', 1)[1].strip().strip('"').strip("'")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    all_pages.append({
                        'title': title,
                        'category': 'database',
                        'type': 'page'
                    })
    def _determine_emotional_context(self, title: str, category: str) -> EmotionalContext:
        title_lower = title.lower()
        category_lower = category.lower()
        if any(word in title_lower for word in ['death', 'funeral', 'memorial', 'obituary', 'grief', 'loss']):
            return EmotionalContext.LOSS_PROCESSING
        elif any(word in title_lower for word in ['executor', 'legal', 'will', 'testament', 'probate']):
            return EmotionalContext.PROACTIVE_PLANNING
        elif any(word in title_lower for word in ['family', 'children', 'legacy', 'heritage', 'memory']):
            return EmotionalContext.CELEBRATION
        elif any(word in title_lower for word in ['comfort', 'support', 'guidance', 'help']):
            return EmotionalContext.HEALTH_CONCERN
        elif any(word in title_lower for word in ['celebration', 'life', 'joy', 'gratitude', 'thankful']):
            return EmotionalContext.CELEBRATION
        elif 'admin' in category_lower:
            return EmotionalContext.PROACTIVE_PLANNING
        else:
            return EmotionalContext.PROACTIVE_PLANNING
    def _generate_enhanced_icon_prompt(self, title: str, category: str, page_type: str, visual_tier: VisualTier, emotional_context: EmotionalContext) -> str:
        return self.prompt_manager.create_prompt(
            title=title,
            category=category,
            asset_type='icon',
            tier=visual_tier.value,
            custom_elements={
                'emotional_context': emotional_context.value,
                'page_type': page_type,
                'luxury_tier': visual_tier.value
            }
        )
    def _generate_enhanced_cover_prompt(self, title: str, category: str, page_type: str, visual_tier: VisualTier, emotional_context: EmotionalContext) -> str:
        return self.prompt_manager.create_prompt(
            title=title,
            category=category,
            asset_type='cover',
            tier=visual_tier.value,
            custom_elements={
                'emotional_context': emotional_context.value,
                'page_type': page_type,
                'visual_tier': visual_tier.value,
                'dimensions': '1500x400px',
                'composition': 'cinematic banner'
            }
        )
    def _generate_enhanced_letter_header_prompt(self, title: str, emotional_context: EmotionalContext) -> str:
        return self.prompt_manager.create_prompt(
            title=title,
            category='letters',
            asset_type='letter_header',
            custom_elements={
                'emotional_context': emotional_context.value,
                'dimensions': '1920x400px',
                'style': 'formal letterhead'
            }
        )
    def _determine_visual_tier(self, title: str, section: str) -> VisualTier:
        return self.hierarchy_manager.determine_visual_tier(title, section, 'icon')
    def _generate_enhanced_prompt(self, title: str, asset_type: str, section: str) -> str:
        visual_tier = self._determine_visual_tier(title, section)
        emotional_context = self._determine_emotional_context(title, section)
        page_type = 'hub' if 'hub' in title.lower() else section
        if asset_type == 'icon':
            return self._generate_enhanced_icon_prompt(
                title, section, page_type, visual_tier, emotional_context
            )
        elif asset_type == 'cover':
            return self._generate_enhanced_cover_prompt(
                title, section, page_type, visual_tier, emotional_context
            )
        elif asset_type == 'letter_header':
            return self._generate_enhanced_letter_header_prompt(
                title, emotional_context
            )
        else:
            return self.prompt_manager.create_prompt(
                title=title,
                category=section,
                asset_type=asset_type,
                tier=visual_tier.value,
                custom_elements={
                    'emotional_context': emotional_context.value,
                    'page_type': page_type,
                    'luxury_tier': visual_tier.value
                }
            )
    def _get_enhanced_textures(self) -> List[Dict]:
        textures = []
        luxury_textures = [
            {"title": "Blueprint Grid", "slug": "blueprint-grid", "category": "technical"},
            {"title": "Parchment", "slug": "parchment", "category": "heritage"},
            {"title": "Legal Pad", "slug": "legal-pad", "category": "professional"},
            {"title": "Marble", "slug": "marble", "category": "luxury"},
            {"title": "Mahogany Grain", "slug": "mahogany-grain", "category": "executive"},
            {"title": "Italian Leather", "slug": "italian-leather", "category": "luxury"},
            {"title": "Belgian Lace", "slug": "belgian-lace", "category": "heritage"},
            {"title": "Gold Leaf Watermark", "slug": "gold-watermark", "category": "premium"},
            {"title": "Engineering Vellum", "slug": "engineering-vellum", "category": "technical"},
            {"title": "Venetian Silk", "slug": "venetian-silk", "category": "luxury"}
        ]
        for texture in luxury_textures:
            enhanced_prompt = self.prompt_manager.create_prompt(
                title=texture['title'],
                category=texture['category'],
                asset_type='texture',
                custom_elements={
                    'seamless': True,
                    'luxury_materials': True,
                    'high_resolution': '512x512px',
                    'texture_category': texture['category']
                }
            )
            textures.append({
                'title': texture['title'],
                'slug': texture['slug'],
                'category': texture['category'],
                'prompt': enhanced_prompt
            })
        return textures
    def _get_enhanced_database_icons(self) -> List[Dict]:
        db_icons = []
        database_categories = [
            {"title": "Financial Accounts", "slug": "db-accounts", "category": "financial"},
            {"title": "Property Records", "slug": "db-properties", "category": "property"},
            {"title": "Insurance Policies", "slug": "db-insurance", "category": "insurance"},
            {"title": "Professional Contacts", "slug": "db-contacts", "category": "contacts"},
            {"title": "Legal Documents", "slug": "db-documents", "category": "legal"},
            {"title": "Family Keepsakes", "slug": "db-keepsakes", "category": "family"},
            {"title": "Executor Tasks", "slug": "db-tasks", "category": "executor"},
            {"title": "Correspondence", "slug": "db-letters", "category": "letters"},
            {"title": "Digital Assets", "slug": "db-digital", "category": "digital"},
            {"title": "Service Subscriptions", "slug": "db-subscriptions", "category": "subscriptions"}
        ]
        for db_category in database_categories:
            visual_tier = VisualTier.TIER_3_DOCUMENT
            emotional_context = self._determine_emotional_context(db_category['title'], db_category['category'])
            enhanced_prompt = self.prompt_manager.create_prompt(
                title=db_category['title'],
                category=db_category['category'],
                asset_type='icon',
                tier=visual_tier.value,
                custom_elements={
                    'database_category': True,
                    'emotional_context': emotional_context.value,
                    'data_organization': True,
                    'professional_aesthetic': True
                }
            )
            db_icons.append({
                'title': db_category['title'],
                'slug': db_category['slug'],
                'category': db_category['category'],
                'visual_tier': visual_tier.value,
                'emotional_context': emotional_context.value,
                'prompt': enhanced_prompt
            })
        return db_icons
def sync_with_yaml() -> Dict[str, List[Dict]]:
    import logging
    from pathlib import Path
    if not logging.getLogger().handlers:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
    yaml_dir = Path(__file__).parent.parent / 'split_yaml'
    sync = YAMLSyncComprehensive(str(yaml_dir))
    logger = logging.getLogger('UltraPremiumSync')
    logger.info("🚀 Starting ultra-premium YAML synchronization...")
    try:
        result = sync.sync_with_yaml()
        logger.info("✨ Ultra-premium prompt generation complete!")
        return result
    except Exception as e:
        logger.error(f"❌ Ultra-premium sync failed: {e}")
        raise
if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger('UltraPremiumTest')
    logger.info("🏆 Testing Ultra-Premium Estate Planning Asset Generation System 🏆")
    try:
        sync = YAMLSyncComprehensive()
        pages = sync.sync_with_yaml()
        total = sum(len(v) for v in pages.values())
        logger.info(f"\n✨ ULTRA-PREMIUM SYNC COMPLETE ✨")
        logger.info(f"Total luxury assets generated: {total}")
        logger.info(f"Estimated cost (premium generation): ${total * 0.08:.2f}")
        logger.info("\n🔍 Sample Enhanced Prompts:")
        if pages.get('icons'):
            logger.info(f"Icon sample: {pages['icons'][0]['prompt'][:100]}...")
        if pages.get('covers'):
            logger.info(f"Cover sample: {pages['covers'][0]['prompt'][:100]}...")
        logger.info("\n📊 Enhanced Features Applied:")
        logger.info("  ✓ 5-tier visual hierarchy")
        logger.info("  ✓ Emotional intelligence integration")
        logger.info("  ✓ Luxury aesthetic enhancement")
        logger.info("  ✓ Section-specific styling")
        logger.info("  ✓ Ultra-premium materials & textures")
    except Exception as e:
        logger.error(f"❌ Ultra-premium test failed: {e}")
        raise

================================================================================
FILE: asset_generation/review_server.py
================================================================================

#!/usr/bin/env python3
import os
import json
import time
import socket
import asyncio
import webbrowser
from pathlib import Path
from http.server import HTTPServer, SimpleHTTPRequestHandler
from threading import Thread
from datetime import datetime
from colorama import init, Fore, Style
init(autoreset=True)
class ReviewRequestHandler(SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/':
            self.path = '/review.html'
        elif self.path == '/status':
            self.send_json_response(self.get_status())
            return
        elif self.path == '/manifest':
            self.send_manifest()
            return
        return SimpleHTTPRequestHandler.do_GET(self)
    def do_POST(self):
        if self.path == '/approve':
            self.handle_approval()
        elif self.path == '/reject':
            self.handle_rejection()
        elif self.path == '/save-prompt':
            self.handle_save_prompt()
        elif self.path == '/regenerate':
            self.handle_regenerate()
        else:
            self.send_error(404)
    def get_status(self):
        manifest_path = Path(self.server.directory) / "manifest.json"
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
                return {
                    'status': 'ready',
                    'samples': len(manifest.get('samples', [])),
                    'total_cost': manifest.get('total_cost', 0),
                    'errors': len(manifest.get('errors', [])),
                    'timestamp': manifest.get('timestamp', '')
                }
        return {'status': 'waiting', 'message': 'No samples generated yet'}
    def send_manifest(self):
        manifest_path = Path(self.server.directory) / "manifest.json"
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                data = json.load(f)
                self.send_json_response(data)
        else:
            self.send_error(404, "Manifest not found")
    def send_json_response(self, data):
        response = json.dumps(data).encode('utf-8')
        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Content-Length', str(len(response)))
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(response)
    def handle_approval(self):
        approval_file = Path("APPROVED.txt")
        with open(approval_file, 'w') as f:
            f.write(f"Samples approved at {datetime.now().isoformat()}\n")
            f.write(f"Reviewed via web interface on port {self.server.server_port}\n")
        self.send_json_response({'status': 'approved', 'message': 'Samples approved successfully'})
        print(f"{Fore.GREEN}✅ Samples APPROVED via web interface{Style.RESET_ALL}")
    def handle_rejection(self):
        rejection_file = Path("REJECTED.txt")
        with open(rejection_file, 'w') as f:
            f.write(f"Samples rejected at {datetime.now().isoformat()}\n")
            f.write(f"Reviewed via web interface on port {self.server.server_port}\n")
        self.send_json_response({'status': 'rejected', 'message': 'Samples rejected'})
        print(f"{Fore.YELLOW}⚠️ Samples REJECTED via web interface{Style.RESET_ALL}")
    def handle_save_prompt(self):
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)
        data = json.loads(post_data.decode('utf-8'))
        prompts_path = Path("../prompts.json")
        if prompts_path.exists():
            with open(prompts_path, 'r') as f:
                prompts_data = json.load(f)
            asset_type = data.get('type')
            page_title = data.get('page_title')
            new_prompt = data.get('prompt')
            if asset_type in prompts_data['prompts']:
                if page_title in prompts_data['prompts'][asset_type]:
                    old_prompt = prompts_data['prompts'][asset_type][page_title]['current']
                    prompts_data['prompts'][asset_type][page_title]['current'] = new_prompt
                    prompts_data['prompts'][asset_type][page_title]['metadata']['prompt_history'].append(new_prompt)
                    if asset_type not in prompts_data['edited_prompts']:
                        prompts_data['edited_prompts'][asset_type] = {}
                    prompts_data['edited_prompts'][asset_type][page_title] = {
                        'old': old_prompt,
                        'new': new_prompt,
                        'edited_at': datetime.now().isoformat()
                    }
                    prompts_data['statistics']['total_prompts_edited'] += 1
                    prompts_data['statistics']['last_updated'] = datetime.now().isoformat()
            with open(prompts_path, 'w') as f:
                json.dump(prompts_data, f, indent=2)
            self.send_json_response({'status': 'saved', 'message': 'Prompt updated successfully'})
            print(f"{Fore.GREEN}✅ Prompt updated for {page_title}{Style.RESET_ALL}")
        else:
            self.send_json_response({'status': 'error', 'message': 'prompts.json not found'})
    def handle_regenerate(self):
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)
        data = json.loads(post_data.decode('utf-8'))
        prompts_path = Path("../prompts.json")
        if prompts_path.exists():
            with open(prompts_path, 'r') as f:
                prompts_data = json.load(f)
            prompts_data['regeneration_queue'].append({
                'type': data.get('type'),
                'page_title': data.get('page_title'),
                'filename': data.get('filename'),
                'prompt': data.get('prompt'),
                'requested_at': datetime.now().isoformat()
            })
            with open(prompts_path, 'w') as f:
                json.dump(prompts_data, f, indent=2)
            self.send_json_response({'status': 'queued', 'message': 'Asset queued for regeneration'})
            print(f"{Fore.CYAN}🔄 Asset queued for regeneration: {data.get('filename')}{Style.RESET_ALL}")
        else:
            self.send_json_response({'status': 'error', 'message': 'prompts.json not found'})
    def log_message(self, format, *args):
        pass
class ReviewServer:
    def __init__(self, port=4500, directory="output/samples", auto_open=True):
        self.port = port
        self.directory = Path(directory)
        self.auto_open = auto_open
        self.server = None
        self.thread = None
        config_path = Path("config.json")
        if config_path.exists():
            with open(config_path) as f:
                config = json.load(f)
                self.port = config.get("review", {}).get("port", 4500)
                self.auto_open = config.get("review", {}).get("auto_open", True)
        self.create_review_html()
    def create_review_html(self):
        html_content =
        html_path = self.directory / "review.html"
        html_path.parent.mkdir(parents=True, exist_ok=True)
        with open(html_path, 'w') as f:
            f.write(html_content)
    def check_port(self, port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(('', port))
                return True
            except:
                return False
    def find_available_port(self, start_port=4500):
        for port in range(start_port, start_port + 10):
            if self.check_port(port):
                return port
        return None
    def start(self):
        if not self.check_port(self.port):
            print(f"{Fore.YELLOW}⚠️ Port {self.port} is in use, finding alternative...{Style.RESET_ALL}")
            self.port = self.find_available_port(self.port)
            if not self.port:
                raise RuntimeError("No available ports found")
            print(f"{Fore.GREEN}✓ Using port {self.port}{Style.RESET_ALL}")
        os.chdir(self.directory)
        handler = ReviewRequestHandler
        handler.directory = self.directory
        self.server = HTTPServer(('localhost', self.port), handler)
        self.server.directory = self.directory
        self.thread = Thread(target=self.server.serve_forever)
        self.thread.daemon = True
        self.thread.start()
        url = f"http://localhost:{self.port}/review.html"
        print(f"\n{Fore.GREEN}✅ Review server started{Style.RESET_ALL}")
        print(f"📍 URL: {Fore.CYAN}{url}{Style.RESET_ALL}")
        if self.auto_open:
            time.sleep(1)
            print(f"{Fore.GREEN}🌐 Opening browser automatically...{Style.RESET_ALL}")
            webbrowser.open(url)
    def stop(self):
        if self.server:
            self.server.shutdown()
            print(f"{Fore.YELLOW}Review server stopped{Style.RESET_ALL}")
    async def wait_for_approval(self, approval_file, timeout=600):
        start_time = time.time()
        approval_path = Path(approval_file)
        rejection_path = Path("REJECTED.txt")
        print(f"\n{Fore.YELLOW}⏳ Waiting for review decision...{Style.RESET_ALL}")
        print(f"   • To approve: Click 'Approve for Production' in browser")
        print(f"   • To reject: Click 'Reject & Refine' in browser")
        print(f"   • Or create {approval_file} manually to approve")
        while time.time() - start_time < timeout:
            if approval_path.exists():
                return True
            if rejection_path.exists():
                return False
            await asyncio.sleep(1)
        print(f"{Fore.RED}⏰ Review timeout after {timeout} seconds{Style.RESET_ALL}")
        return False
def launch_review_after_generation():
    server = ReviewServer()
    server.start()
    print(f"\n{Fore.CYAN}Press Enter to stop the review server...{Style.RESET_ALL}")
    input()
    server.stop()
    if Path("APPROVED.txt").exists():
        return True
    return False
if __name__ == "__main__":
    server = ReviewServer()
    server.start()
    try:
        print(f"\n{Fore.CYAN}Review server running. Press Ctrl+C to stop.{Style.RESET_ALL}")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        server.stop()

================================================================================
FILE: asset_generation/review_dashboard.py
================================================================================

#!/usr/bin/env python3
import os
import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import logging
try:
    from flask import Flask, render_template, request, jsonify, send_from_directory
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    print("Flask not available. Install with: pip install flask flask-cors")
from openrouter_orchestrator import OpenRouterOrchestrator, PromptCompetition
from quality_scorer import QualityScorer, CompetitiveEvaluation
from sample_generator import SampleGenerator
from sync_yaml_comprehensive import YAMLSyncComprehensive
@dataclass
class ReviewSession:
    session_id: str
    reviewer_name: str
    start_time: str
    pages_reviewed: int
    decisions_made: int
    quality_feedback: List[Dict[str, Any]]
    session_notes: str
    completion_status: str
@dataclass
class HumanDecision:
    page_title: str
    page_category: str
    asset_type: str
    selected_prompt_id: str
    selected_model: str
    decision_reasoning: str
    quality_override: Optional[float] = None
    custom_modifications: Optional[str] = None
    decision_timestamp: str = None
    def __post_init__(self):
        if self.decision_timestamp is None:
            self.decision_timestamp = datetime.now().isoformat()
class ReviewDashboard:
    def __init__(self, port: int = 5000):
        if not FLASK_AVAILABLE:
            raise ImportError("Flask is required. Install with: pip install flask flask-cors")
        self.port = port
        self.app = Flask(__name__, template_folder='templates', static_folder='static')
        CORS(self.app)
        self.orchestrator = OpenRouterOrchestrator()
        self.quality_scorer = QualityScorer()
        self.sample_generator = SampleGenerator()
        self.yaml_system = YAMLSyncComprehensive()
        self.logger = self._setup_logger()
        self.current_session: Optional[ReviewSession] = None
        self.human_decisions: List[HumanDecision] = []
        self.competitive_evaluations: List[CompetitiveEvaluation] = []
        self._setup_routes()
        self._create_templates()
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger('ReviewDashboard')
        logger.setLevel(logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        fh = logging.FileHandler('review_dashboard.log')
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        return logger
    def _setup_routes(self):
        @self.app.route('/')
        def index():
            return render_template('dashboard.html',
                                 session=self.current_session,
                                 total_evaluations=len(self.competitive_evaluations),
                                 decisions_made=len(self.human_decisions))
        @self.app.route('/api/start-session', methods=['POST'])
        def start_session():
            data = request.json
            reviewer_name = data.get('reviewer_name', 'Anonymous')
            self.current_session = ReviewSession(
                session_id=f"review_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                reviewer_name=reviewer_name,
                start_time=datetime.now().isoformat(),
                pages_reviewed=0,
                decisions_made=0,
                quality_feedback=[],
                session_notes="",
                completion_status='active'
            )
            self.logger.info(f"Started review session: {self.current_session.session_id}")
            return jsonify({'success': True, 'session': asdict(self.current_session)})
        @self.app.route('/api/load-evaluations', methods=['POST'])
        def load_evaluations():
            try:
                data = request.json
                file_path = data.get('file_path', 'quality_evaluation_results.json')
                if Path(file_path).exists():
                    with open(file_path, 'r') as f:
                        results_data = json.load(f)
                    self.competitive_evaluations = self._parse_evaluation_results(results_data)
                    return jsonify({
                        'success': True,
                        'evaluations_loaded': len(self.competitive_evaluations),
                        'message': f'Loaded {len(self.competitive_evaluations)} competitive evaluations'
                    })
                else:
                    return jsonify({'success': False, 'error': f'File not found: {file_path}'}), 404
            except Exception as e:
                self.logger.error(f"Failed to load evaluations: {e}")
                return jsonify({'success': False, 'error': str(e)}), 500
        @self.app.route('/api/get-evaluation/<int:index>')
        def get_evaluation(index):
            if 0 <= index < len(self.competitive_evaluations):
                evaluation = self.competitive_evaluations[index]
                eval_data = {
                    'index': index,
                    'page_title': evaluation.page_title,
                    'page_category': evaluation.page_category,
                    'asset_type': evaluation.asset_type,
                    'prompts': [
                        {
                            'id': pe.prompt_id,
                            'text': pe.prompt_text,
                            'model_source': pe.model_source,
                            'overall_score': pe.overall_score,
                            'weighted_score': pe.weighted_score,
                            'score_breakdown': pe.score_breakdown
                        }
                        for pe in evaluation.prompt_evaluations
                    ],
                    'winner': {
                        'id': evaluation.winner.prompt_id,
                        'model_source': evaluation.winner.model_source,
                        'score': evaluation.winner.weighted_score
                    } if evaluation.winner else None,
                    'consensus_scores': evaluation.consensus_scores,
                    'evaluation_summary': evaluation.evaluation_summary
                }
                return jsonify({'success': True, 'evaluation': eval_data})
            else:
                return jsonify({'success': False, 'error': 'Invalid evaluation index'}), 400
        @self.app.route('/api/make-decision', methods=['POST'])
        def make_decision():
            try:
                data = request.json
                decision = HumanDecision(
                    page_title=data['page_title'],
                    page_category=data['page_category'],
                    asset_type=data['asset_type'],
                    selected_prompt_id=data['selected_prompt_id'],
                    selected_model=data['selected_model'],
                    decision_reasoning=data.get('reasoning', ''),
                    quality_override=data.get('quality_override'),
                    custom_modifications=data.get('custom_modifications')
                )
                self.human_decisions.append(decision)
                if self.current_session:
                    self.current_session.decisions_made += 1
                    self.current_session.pages_reviewed += 1
                self.logger.info(f"Decision recorded: {decision.page_title} -> {decision.selected_model}")
                return jsonify({
                    'success': True,
                    'decision_id': len(self.human_decisions) - 1,
                    'total_decisions': len(self.human_decisions)
                })
            except Exception as e:
                self.logger.error(f"Failed to record decision: {e}")
                return jsonify({'success': False, 'error': str(e)}), 500
        @self.app.route('/api/get-progress')
        def get_progress():
            total_evaluations = len(self.competitive_evaluations)
            decisions_made = len(self.human_decisions)
            progress_data = {
                'total_evaluations': total_evaluations,
                'decisions_made': decisions_made,
                'completion_percentage': (decisions_made / total_evaluations * 100) if total_evaluations > 0 else 0,
                'session': asdict(self.current_session) if self.current_session else None
            }
            return jsonify(progress_data)
        @self.app.route('/api/export-decisions')
        def export_decisions():
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'session': asdict(self.current_session) if self.current_session else None,
                'total_decisions': len(self.human_decisions),
                'decisions': [asdict(decision) for decision in self.human_decisions],
                'decision_summary': self._generate_decision_summary()
            }
            output_file = f"human_decisions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w') as f:
                json.dump(export_data, f, indent=2)
            self.logger.info(f"Exported {len(self.human_decisions)} decisions to {output_file}")
            return jsonify({
                'success': True,
                'file_path': output_file,
                'total_decisions': len(self.human_decisions)
            })
        @self.app.route('/api/generate-final-prompts')
        def generate_final_prompts():
            final_prompts = {}
            for decision in self.human_decisions:
                key = f"{decision.page_title}_{decision.asset_type}"
                selected_prompt = self._find_prompt_by_decision(decision)
                if selected_prompt:
                    final_prompts[key] = {
                        'page_title': decision.page_title,
                        'page_category': decision.page_category,
                        'asset_type': decision.asset_type,
                        'selected_prompt': selected_prompt,
                        'human_reasoning': decision.decision_reasoning,
                        'custom_modifications': decision.custom_modifications,
                        'decision_timestamp': decision.decision_timestamp
                    }
            output_file = f"final_selected_prompts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w') as f:
                json.dump(final_prompts, f, indent=2)
            self.logger.info(f"Generated {len(final_prompts)} final prompt selections")
            return jsonify({
                'success': True,
                'final_prompts_file': output_file,
                'total_selections': len(final_prompts)
            })
    def _parse_evaluation_results(self, results_data: Dict[str, Any]) -> List[CompetitiveEvaluation]:
        evaluations = []
        for eval_data in results_data.get('competitive_evaluations', []):
            evaluation = type('CompetitiveEvaluation', (), eval_data)()
            evaluations.append(evaluation)
        return evaluations
    def _find_prompt_by_decision(self, decision: HumanDecision) -> Optional[str]:
        for evaluation in self.competitive_evaluations:
            if (evaluation.page_title == decision.page_title and
                evaluation.asset_type == decision.asset_type):
                for prompt_eval in evaluation.prompt_evaluations:
                    if prompt_eval.prompt_id == decision.selected_prompt_id:
                        return prompt_eval.prompt_text
        return None
    def _generate_decision_summary(self) -> Dict[str, Any]:
        if not self.human_decisions:
            return {}
        model_preferences = {}
        asset_type_counts = {}
        for decision in self.human_decisions:
            model_preferences[decision.selected_model] = model_preferences.get(decision.selected_model, 0) + 1
            asset_type_counts[decision.asset_type] = asset_type_counts.get(decision.asset_type, 0) + 1
        return {
            'total_decisions': len(self.human_decisions),
            'model_preference_ranking': sorted(model_preferences.items(), key=lambda x: x[1], reverse=True),
            'asset_type_distribution': asset_type_counts,
            'decisions_with_custom_modifications': sum(1 for d in self.human_decisions if d.custom_modifications),
            'decisions_with_quality_override': sum(1 for d in self.human_decisions if d.quality_override),
            'average_decision_reasoning_length': sum(len(d.decision_reasoning) for d in self.human_decisions) / len(self.human_decisions)
        }
    def _create_templates(self):
        templates_dir = Path('templates')
        templates_dir.mkdir(exist_ok=True)
        dashboard_html =
        with open(templates_dir / 'dashboard.html', 'w') as f:
            f.write(dashboard_html)
        self.logger.info("Created dashboard template")
    def run(self, debug: bool = True):
        self.logger.info(f"Starting Review Dashboard on http://localhost:{self.port}")
        print(f"\n🌐 Estate Planning Concierge v4.0 - Review Dashboard")
        print(f"📊 Open http://localhost:{self.port} to start reviewing prompts")
        print(f"🎯 Use this interface to review AI-generated prompts and make final selections")
        self.app.run(host='0.0.0.0', port=self.port, debug=debug)
def create_dashboard_server(port: int = 5000):
    try:
        dashboard = ReviewDashboard(port=port)
        return dashboard
    except ImportError as e:
        print(f"Cannot create dashboard: {e}")
        print("Install required dependencies: pip install flask flask-cors")
        return None
async def test_review_dashboard():
    print("🎛️ Testing Review Dashboard...")
    dashboard = create_dashboard_server(port=5001)
    if dashboard:
        print(f"✅ Review dashboard created successfully!")
        print(f"📱 Dashboard features:")
        print(f"  - Interactive web interface for prompt review")
        print(f"  - Side-by-side prompt comparison")
        print(f"  - Quality score visualization")
        print(f"  - Human decision recording")
        print(f"  - Progress tracking")
        print(f"  - Export capabilities")
        print(f"\n🚀 To run the dashboard:")
        print(f"  dashboard = create_dashboard_server()")
        print(f"  dashboard.run()")
        print(f"
        return True
    else:
        print("❌ Dashboard creation failed")
        return False
if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        asyncio.run(test_review_dashboard())
    else:
        dashboard = create_dashboard_server()
        if dashboard:
            dashboard.run()
        else:
            print("Failed to create dashboard. Check dependencies.")

================================================================================
FILE: asset_generation/git_operations.py
================================================================================

#!/usr/bin/env python3
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
logger = logging.getLogger(__name__)
class GitOperations:
    def __init__(self, repo_path: Path = None):
        self.repo_path = repo_path or Path.cwd()
    def check_git_repo(self) -> bool:
        try:
            result = subprocess.run(
                ['git', 'rev-parse', '--git-dir'],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=False
            )
            return result.returncode == 0
        except FileNotFoundError:
            logger.error("Git is not installed or not in PATH")
            return False
        except Exception as e:
            logger.error(f"Error checking Git repository: {e}")
            return False
    def get_current_branch(self) -> Optional[str]:
        try:
            result = subprocess.run(
                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=True
            )
            return result.stdout.strip()
        except Exception as e:
            logger.error(f"Error getting current branch: {e}")
            return None
    def has_uncommitted_changes(self) -> bool:
        try:
            result = subprocess.run(
                ['git', 'status', '--porcelain'],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=True
            )
            return bool(result.stdout.strip())
        except Exception as e:
            logger.error(f"Error checking Git status: {e}")
            return False
    def stage_assets(self, asset_dir: str = "assets") -> Tuple[bool, List[str]]:
        staged_files = []
        try:
            asset_path = self.repo_path / asset_dir
            if not asset_path.exists():
                logger.warning(f"Asset directory {asset_path} does not exist")
                return False, []
            result = subprocess.run(
                ['git', 'add', asset_dir],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=False
            )
            if result.returncode != 0:
                logger.error(f"Failed to stage assets: {result.stderr}")
                return False, []
            result = subprocess.run(
                ['git', 'diff', '--cached', '--name-only', asset_dir],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=True
            )
            staged_files = [f for f in result.stdout.strip().split('\n') if f]
            logger.info(f"Staged {len(staged_files)} files from {asset_dir}")
            return True, staged_files
        except Exception as e:
            logger.error(f"Error staging assets: {e}")
            return False, []
    def create_commit(self, message: str, body: str = "") -> bool:
        try:
            result = subprocess.run(
                ['git', 'diff', '--cached', '--quiet'],
                cwd=self.repo_path,
                capture_output=True,
                check=False
            )
            if result.returncode == 0:
                logger.info("No staged changes to commit")
                return True
            commit_message = message
            if body:
                commit_message = f"{message}\n\n{body}"
            result = subprocess.run(
                ['git', 'commit', '-m', commit_message],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=False
            )
            if result.returncode != 0:
                logger.error(f"Failed to create commit: {result.stderr}")
                return False
            logger.info(f"Created commit: {message}")
            return True
        except Exception as e:
            logger.error(f"Error creating commit: {e}")
            return False
    def push_to_remote(self, remote: str = "origin", branch: str = None) -> bool:
        try:
            if branch is None:
                branch = self.get_current_branch()
                if not branch:
                    logger.error("Could not determine current branch")
                    return False
            result = subprocess.run(
                ['git', 'remote', 'get-url', remote],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=False
            )
            if result.returncode != 0:
                logger.warning(f"Remote '{remote}' not configured, skipping push")
                return True
            logger.info(f"Pushing to {remote}/{branch}...")
            result = subprocess.run(
                ['git', 'push', remote, branch],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=False
            )
            if result.returncode != 0:
                logger.error(f"Failed to push: {result.stderr}")
                return False
            logger.info(f"Successfully pushed to {remote}/{branch}")
            return True
        except Exception as e:
            logger.error(f"Error pushing to remote: {e}")
            return False
    def auto_commit_assets(self, stats: Dict, mode: str = "production",
                          dry_run: bool = False) -> bool:
        try:
            if not self.check_git_repo():
                logger.warning("Not in a Git repository, skipping auto-commit")
                return True
            success, staged_files = self.stage_assets()
            if not success:
                logger.error("Failed to stage assets")
                return False
            if not staged_files:
                logger.info("No new assets to commit")
                return True
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            icons_count = stats.get('icons_generated', 0)
            covers_count = stats.get('covers_generated', 0)
            textures_count = stats.get('textures_generated', 0)
            total_count = icons_count + covers_count + textures_count
            total_cost = stats.get('total_cost', 0.0)
            title = f"feat(assets): Generate {total_count} {mode} assets"
            body_lines = [
                f"Generated on: {timestamp}",
                f"Mode: {mode.capitalize()}",
                "",
                "Assets generated:",
                f"  - Icons: {icons_count}",
                f"  - Covers: {covers_count}",
                f"  - Textures: {textures_count}",
                f"  - Total: {total_count}",
                "",
                f"Cost: ${total_cost:.2f}",
                f"Files changed: {len(staged_files)}",
            ]
            if stats.get('regenerated_count'):
                body_lines.append(f"Regenerated: {stats['regenerated_count']} assets")
            body = '\n'.join(body_lines)
            if dry_run:
                logger.info("DRY RUN - Would commit with message:")
                logger.info(f"{title}\n\n{body}")
                logger.info(f"Files to be committed: {len(staged_files)}")
                return True
            if not self.create_commit(title, body):
                logger.error("Failed to create commit")
                return False
            if not self.push_to_remote():
                logger.warning("Commit created but push failed - you may need to push manually")
            logger.info(f"Successfully committed {len(staged_files)} asset files")
            return True
        except Exception as e:
            logger.error(f"Error in auto-commit process: {e}")
            return False
    def get_last_asset_commit(self) -> Optional[str]:
        try:
            result = subprocess.run(
                ['git', 'log', '--grep=feat(assets):', '-1', '--format=%H'],
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=True
            )
            commit_sha = result.stdout.strip()
            return commit_sha if commit_sha else None
        except Exception as e:
            logger.error(f"Error getting last asset commit: {e}")
            return None

================================================================================
FILE: asset_generation/quality_scorer.py
================================================================================

#!/usr/bin/env python3
import os
import json
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from pathlib import Path
from enum import Enum
class ScoringCriterion(Enum):
    EMOTIONAL_INTELLIGENCE = "emotional_intelligence"
    LUXURY_AESTHETIC = "luxury_aesthetic"
    TECHNICAL_CLARITY = "technical_clarity"
    VISUAL_CONSISTENCY = "visual_consistency"
    INNOVATION = "innovation"
    ESTATE_PLANNING_RELEVANCE = "estate_planning_relevance"
    BRAND_COHERENCE = "brand_coherence"
@dataclass
class QualityScore:
    criterion: ScoringCriterion
    score: float
    reasoning: str
    strengths: List[str]
    weaknesses: List[str]
    suggestions: List[str]
@dataclass
class PromptEvaluation:
    prompt_id: str
    prompt_text: str
    model_source: str
    category: str
    asset_type: str
    individual_scores: List[QualityScore]
    overall_score: float
    weighted_score: float
    evaluation_timestamp: str
    evaluator_model: str
    @property
    def score_breakdown(self) -> Dict[str, float]:
        return {score.criterion.value: score.score for score in self.individual_scores}
@dataclass
class CompetitiveEvaluation:
    page_title: str
    page_category: str
    asset_type: str
    prompt_evaluations: List[PromptEvaluation]
    winner: Optional[PromptEvaluation] = None
    consensus_scores: Optional[Dict[str, float]] = None
    evaluation_summary: Optional[str] = None
class QualityScorer:
    def __init__(self, openrouter_api_key: str = None):
        self.api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        if not self.api_key:
            raise ValueError("OpenRouter API key is required")
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.logger = self._setup_logger()
        self.scoring_weights = {
            ScoringCriterion.EMOTIONAL_INTELLIGENCE: 0.25,
            ScoringCriterion.LUXURY_AESTHETIC: 0.20,
            ScoringCriterion.TECHNICAL_CLARITY: 0.15,
            ScoringCriterion.VISUAL_CONSISTENCY: 0.15,
            ScoringCriterion.INNOVATION: 0.10,
            ScoringCriterion.ESTATE_PLANNING_RELEVANCE: 0.10,
            ScoringCriterion.BRAND_COHERENCE: 0.05
        }
        self.evaluator_models = {
            'detailed_analyzer': {
                'id': 'anthropic/claude-3-opus-20240229',
                'perspective': 'detailed_analysis',
                'strengths': ['thorough_evaluation', 'nuanced_feedback', 'estate_planning_expertise']
            },
            'luxury_expert': {
                'id': 'openai/gpt-4-turbo-preview',
                'perspective': 'luxury_assessment',
                'strengths': ['luxury_brand_expertise', 'visual_aesthetics', 'premium_positioning']
            },
            'technical_validator': {
                'id': 'google/gemini-pro',
                'perspective': 'technical_validation',
                'strengths': ['technical_precision', 'consistency_checking', 'implementation_feasibility']
            }
        }
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger('QualityScorer')
        logger.setLevel(logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        fh = logging.FileHandler('quality_scoring.log')
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        return logger
    async def _call_evaluator_model(self, model_id: str, prompt: str, temperature: float = 0.3) -> Dict[str, Any]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://estate-planning-concierge.com",
            "X-Title": "Estate Planning Concierge v4.0 - Quality Evaluation"
        }
        payload = {
            "model": model_id,
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert in luxury brand design and estate planning user experience. Provide detailed, objective evaluations with specific reasoning."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": temperature,
            "max_tokens": 2000
        }
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(self.base_url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return {
                            'success': True,
                            'content': result['choices'][0]['message']['content'],
                            'model': model_id,
                            'usage': result.get('usage', {})
                        }
                    else:
                        error_text = await response.text()
                        self.logger.error(f"Evaluator API error: {response.status} - {error_text}")
                        return {
                            'success': False,
                            'error': f"API error: {response.status}",
                            'model': model_id
                        }
            except Exception as e:
                self.logger.error(f"Evaluator request failed: {str(e)}")
                return {
                    'success': False,
                    'error': str(e),
                    'model': model_id
                }
    def _build_evaluation_prompt(self, prompt_to_evaluate: str, context: Dict[str, Any],
                                evaluator_perspective: str) -> str:
        base_context = f
        if evaluator_perspective == 'detailed_analysis':
            specific_focus =
        elif evaluator_perspective == 'luxury_assessment':
            specific_focus =
        else:
            specific_focus =
        full_prompt = base_context + specific_focus +
        return full_prompt
    async def evaluate_single_prompt(self, prompt_text: str, context: Dict[str, Any],
                                   evaluator_model: str = 'detailed_analyzer') -> PromptEvaluation:
        model_config = self.evaluator_models[evaluator_model]
        evaluation_prompt = self._build_evaluation_prompt(
            prompt_text, context, model_config['perspective']
        )
        result = await self._call_evaluator_model(model_config['id'], evaluation_prompt)
        if not result['success']:
            self.logger.error(f"Failed to evaluate prompt with {evaluator_model}: {result.get('error')}")
            raise Exception(f"Evaluation failed: {result.get('error')}")
        try:
            evaluation_data = json.loads(result['content'])
            individual_scores = []
            for criterion_name, criterion_data in evaluation_data.get('detailed_analysis', {}).items():
                if criterion_name in [c.value for c in ScoringCriterion]:
                    criterion_enum = ScoringCriterion(criterion_name)
                    quality_score = QualityScore(
                        criterion=criterion_enum,
                        score=criterion_data.get('score', 0),
                        reasoning=criterion_data.get('reasoning', ''),
                        strengths=criterion_data.get('strengths', []),
                        weaknesses=criterion_data.get('weaknesses', []),
                        suggestions=criterion_data.get('suggestions', [])
                    )
                    individual_scores.append(quality_score)
            weighted_score = sum(
                score.score * self.scoring_weights.get(score.criterion, 0)
                for score in individual_scores
            )
            prompt_evaluation = PromptEvaluation(
                prompt_id=f"{context['page_title']}_{context['asset_type']}_{evaluator_model}",
                prompt_text=prompt_text,
                model_source=context.get('model_source', 'unknown'),
                category=context['page_category'],
                asset_type=context['asset_type'],
                individual_scores=individual_scores,
                overall_score=evaluation_data.get('overall_assessment', {}).get('overall_score', 0),
                weighted_score=weighted_score,
                evaluation_timestamp=datetime.now().isoformat(),
                evaluator_model=evaluator_model
            )
            self.logger.info(f"Successfully evaluated prompt: {prompt_evaluation.prompt_id} (Score: {weighted_score:.2f})")
            return prompt_evaluation
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse evaluation response: {e}")
            raise Exception(f"Invalid evaluation response format: {e}")
    async def evaluate_competitive_prompts(self, prompts: List[Dict[str, Any]],
                                         context: Dict[str, Any]) -> CompetitiveEvaluation:
        self.logger.info(f"Evaluating {len(prompts)} competitive prompts for {context['page_title']}")
        all_evaluations = []
        for i, prompt_data in enumerate(prompts):
            prompt_context = {
                **context,
                'model_source': prompt_data.get('model_source', f'prompt_{i+1}')
            }
            try:
                evaluation = await self.evaluate_single_prompt(
                    prompt_data['prompt'], prompt_context, 'detailed_analyzer'
                )
                all_evaluations.append(evaluation)
                await asyncio.sleep(0.5)
            except Exception as e:
                self.logger.error(f"Failed to evaluate prompt {i+1}: {e}")
                continue
        if not all_evaluations:
            raise Exception("No prompts could be evaluated successfully")
        winner = max(all_evaluations, key=lambda x: x.weighted_score)
        consensus_scores = {}
        for criterion in ScoringCriterion:
            criterion_scores = []
            for evaluation in all_evaluations:
                for score in evaluation.individual_scores:
                    if score.criterion == criterion:
                        criterion_scores.append(score.score)
            if criterion_scores:
                consensus_scores[criterion.value] = sum(criterion_scores) / len(criterion_scores)
        evaluation_summary = self._generate_evaluation_summary(all_evaluations, winner, consensus_scores)
        competitive_evaluation = CompetitiveEvaluation(
            page_title=context['page_title'],
            page_category=context['page_category'],
            asset_type=context['asset_type'],
            prompt_evaluations=all_evaluations,
            winner=winner,
            consensus_scores=consensus_scores,
            evaluation_summary=evaluation_summary
        )
        self.logger.info(f"Competitive evaluation complete. Winner: {winner.model_source} (Score: {winner.weighted_score:.2f})")
        return competitive_evaluation
    def _generate_evaluation_summary(self, evaluations: List[PromptEvaluation],
                                   winner: PromptEvaluation,
                                   consensus_scores: Dict[str, float]) -> str:
        summary_parts = [
            f"COMPETITIVE EVALUATION SUMMARY",
            f"Total Prompts Evaluated: {len(evaluations)}",
            f"Winner: {winner.model_source} (Weighted Score: {winner.weighted_score:.2f})",
            f"",
            f"CONSENSUS SCORES (Average across all prompts):"
        ]
        for criterion, score in consensus_scores.items():
            summary_parts.append(f"  {criterion.replace('_', ' ').title()}: {score:.2f}/10")
        summary_parts.extend([
            f"",
            f"TOP PERFORMING AREAS:",
            f"  Best: {max(consensus_scores.items(), key=lambda x: x[1])[0].replace('_', ' ').title()} ({max(consensus_scores.values()):.2f}/10)",
            f"",
            f"IMPROVEMENT OPPORTUNITIES:",
            f"  Focus: {min(consensus_scores.items(), key=lambda x: x[1])[0].replace('_', ' ').title()} ({min(consensus_scores.values()):.2f}/10)",
            f"",
            f"WINNER ANALYSIS:",
            f"  Model: {winner.model_source}",
            f"  Overall Score: {winner.overall_score:.2f}/10",
            f"  Weighted Score: {winner.weighted_score:.2f}/10"
        ])
        return "\n".join(summary_parts)
    def save_evaluation_results(self, competitive_evaluations: List[CompetitiveEvaluation],
                              output_file: str = "quality_evaluation_results.json") -> Path:
        output_path = Path(output_file)
        data = {
            'evaluation_metadata': {
                'total_evaluations': len(competitive_evaluations),
                'scoring_criteria': [c.value for c in ScoringCriterion],
                'scoring_weights': {k.value: v for k, v in self.scoring_weights.items()},
                'evaluator_models': self.evaluator_models,
                'evaluation_timestamp': datetime.now().isoformat()
            },
            'competitive_evaluations': [],
            'summary_statistics': self._generate_overall_statistics(competitive_evaluations)
        }
        for comp_eval in competitive_evaluations:
            eval_data = {
                'page_title': comp_eval.page_title,
                'page_category': comp_eval.page_category,
                'asset_type': comp_eval.asset_type,
                'prompt_evaluations': [asdict(pe) for pe in comp_eval.prompt_evaluations],
                'winner': asdict(comp_eval.winner) if comp_eval.winner else None,
                'consensus_scores': comp_eval.consensus_scores,
                'evaluation_summary': comp_eval.evaluation_summary
            }
            data['competitive_evaluations'].append(eval_data)
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        self.logger.info(f"Evaluation results saved to {output_path}")
        return output_path
    def _generate_overall_statistics(self, competitive_evaluations: List[CompetitiveEvaluation]) -> Dict[str, Any]:
        all_scores = []
        winner_models = []
        criterion_averages = {c.value: [] for c in ScoringCriterion}
        for comp_eval in competitive_evaluations:
            if comp_eval.winner:
                all_scores.append(comp_eval.winner.weighted_score)
                winner_models.append(comp_eval.winner.model_source)
            if comp_eval.consensus_scores:
                for criterion, score in comp_eval.consensus_scores.items():
                    if criterion in criterion_averages:
                        criterion_averages[criterion].append(score)
        stats = {
            'overall_quality_metrics': {
                'average_winner_score': sum(all_scores) / len(all_scores) if all_scores else 0,
                'highest_score': max(all_scores) if all_scores else 0,
                'lowest_score': min(all_scores) if all_scores else 0,
                'score_distribution': {
                    'excellent_8_10': sum(1 for s in all_scores if s >= 8),
                    'good_6_8': sum(1 for s in all_scores if 6 <= s < 8),
                    'needs_improvement_0_6': sum(1 for s in all_scores if s < 6)
                }
            },
            'model_performance': {},
            'criterion_analysis': {}
        }
        from collections import Counter
        model_counts = Counter(winner_models)
        stats['model_performance'] = {
            'winner_frequency': dict(model_counts),
            'top_performing_model': model_counts.most_common(1)[0][0] if model_counts else None
        }
        for criterion, scores in criterion_averages.items():
            if scores:
                stats['criterion_analysis'][criterion] = {
                    'average_score': sum(scores) / len(scores),
                    'highest_score': max(scores),
                    'lowest_score': min(scores),
                    'consistency': max(scores) - min(scores)
                }
        return stats
async def test_quality_scorer():
    scorer = QualityScorer()
    test_prompts = [
        {
            'prompt': "Ultra-luxury icon for 'Executor Hub': mahogany law library aesthetic, scales of justice in polished brass, leather-bound book spine texture, three-tier gradient from amber to bronze, floating shadow with gold rim light, ornate serif details, SVG vector art optimized for 24px-256px display",
            'model_source': 'claude_emotional'
        },
        {
            'prompt': "Premium executor hub icon with sophisticated materials: dark mahogany wood paneling, brass legal scales, rich leather textures, golden ambient lighting, executive office atmosphere, professional gravitas, high-end law firm aesthetic, vector art format",
            'model_source': 'gpt4_luxury'
        },
        {
            'prompt': "Technical precision executor icon: mahogany material properties with 15% gloss, brass scales at 45-degree angle, leather texture with 2px embossed grain, 3-point lighting setup, golden ratio composition, SVG vector format optimized for responsive display",
            'model_source': 'gemini_technical'
        }
    ]
    test_context = {
        'page_title': 'Executor Hub',
        'page_category': 'executor',
        'asset_type': 'icon'
    }
    print("🎯 Testing Quality Scorer with Estate Planning prompts...")
    competitive_eval = await scorer.evaluate_competitive_prompts(test_prompts, test_context)
    output_file = scorer.save_evaluation_results([competitive_eval])
    print(f"\n✅ Quality scoring complete!")
    print(f"📊 Evaluated {len(test_prompts)} prompts")
    print(f"🏆 Winner: {competitive_eval.winner.model_source} (Score: {competitive_eval.winner.weighted_score:.2f})")
    print(f"📁 Results saved to: {output_file}")
    print(f"\n📋 Evaluation Summary:")
    print(competitive_eval.evaluation_summary)
    return output_file
if __name__ == "__main__":
    asyncio.run(test_quality_scorer())

================================================================================
FILE: asset_generation/prompt_templates.py
================================================================================

#!/usr/bin/env python3
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import json
from pathlib import Path
class AssetType(Enum):
    ICON = "icon"
    COVER = "cover"
    LETTER_HEADER = "letter_header"
    DATABASE_ICON = "database_icon"
    TEXTURE = "texture"
class PageTier(Enum):
    HUB = "hub"
    SECTION = "section"
    DOCUMENT = "document"
    LETTER = "letter"
    DIGITAL = "digital"
class EmotionalTone(Enum):
    WARM_WELCOME = "warm_welcome"
    TRUSTED_GUIDE = "trusted_guide"
    FAMILY_HERITAGE = "family_heritage"
    SECURE_PROTECTION = "secure_protection"
    PEACEFUL_TRANSITION = "peaceful_transition"
    LIVING_CONTINUITY = "living_continuity"
    TECH_BRIDGE = "tech_bridge"
@dataclass
class StyleElements:
    materials: List[str] = field(default_factory=list)
    lighting: List[str] = field(default_factory=list)
    colors: List[str] = field(default_factory=list)
    textures: List[str] = field(default_factory=list)
    objects: List[str] = field(default_factory=list)
    composition: List[str] = field(default_factory=list)
@dataclass
class EmotionalElements:
    comfort_symbols: List[str] = field(default_factory=list)
    human_touches: List[str] = field(default_factory=list)
    continuity_metaphors: List[str] = field(default_factory=list)
    warmth_markers: List[str] = field(default_factory=list)
    life_elements: List[str] = field(default_factory=list)
@dataclass
class PromptTemplate:
    asset_type: AssetType
    page_tier: PageTier
    emotional_tone: EmotionalTone
    base_description: str
    style_elements: StyleElements
    emotional_elements: EmotionalElements
    technical_specs: Dict[str, Any] = field(default_factory=dict)
    section_theme: Optional[str] = None
    unique_focal_point: Optional[str] = None
class PromptTemplateManager:
    def __init__(self):
        self.templates = {}
        self.section_themes = self._initialize_section_themes()
        self.emotional_mappings = self._initialize_emotional_mappings()
        self.style_library = self._initialize_style_library()
    def _initialize_section_themes(self) -> Dict[str, Dict[str, Any]]:
        return {
            'admin': {
                'aesthetic': 'Mission Control meets Executive Boardroom',
                'palette': ['charcoal', 'platinum', 'electric blue'],
                'materials': ['brushed steel', 'smoked glass', 'carbon fiber'],
                'lighting': 'cool focused LED',
                'emotional_tone': EmotionalTone.TRUSTED_GUIDE
            },
            'executor': {
                'aesthetic': 'Law Library meets Private Study',
                'palette': ['deep mahogany', 'forest green', 'aged brass'],
                'materials': ['leather-bound books', 'wood paneling', 'bronze fixtures'],
                'lighting': 'warm lamplight through amber glass',
                'emotional_tone': EmotionalTone.TRUSTED_GUIDE
            },
            'family': {
                'aesthetic': 'Heritage Estate meets Memory Lane',
                'palette': ['warm oak', 'sage green', 'antique gold'],
                'materials': ['heirloom wood', 'vintage fabrics', 'family silver'],
                'lighting': 'soft golden hour',
                'emotional_tone': EmotionalTone.FAMILY_HERITAGE
            },
            'financial': {
                'aesthetic': 'Private Bank meets Vault Room',
                'palette': ['navy blue', 'champagne gold', 'pearl white'],
                'materials': ['marble columns', 'brass fixtures', 'security glass'],
                'lighting': 'precise clean confidence',
                'emotional_tone': EmotionalTone.SECURE_PROTECTION
            },
            'property': {
                'aesthetic': 'Architect Office meets Estate Grounds',
                'palette': ['blueprint blue', 'surveyor green', 'terra cotta'],
                'materials': ['drafting paper', 'copper pipes', 'estate stone'],
                'lighting': 'natural daylight precision',
                'emotional_tone': EmotionalTone.LIVING_CONTINUITY
            },
            'digital': {
                'aesthetic': 'Traditional Luxury meets Modern Tech',
                'palette': ['mahogany with blue glow', 'gold circuits', 'screen light'],
                'materials': ['wood desk with devices', 'leather with aluminum', 'glass displays'],
                'lighting': 'ambient screen glow on traditional surfaces',
                'emotional_tone': EmotionalTone.TECH_BRIDGE
            },
            'letters': {
                'aesthetic': 'Formal Correspondence meets Personal Touch',
                'palette': ['cream parchment', 'midnight ink', 'wax seal colors'],
                'materials': ['quality paper', 'fountain pen', 'wax seals', 'ribbon'],
                'lighting': 'soft desk lamp on writing surface',
                'emotional_tone': EmotionalTone.PEACEFUL_TRANSITION
            }
        }
    def _initialize_emotional_mappings(self) -> Dict[EmotionalTone, EmotionalElements]:
        return {
            EmotionalTone.WARM_WELCOME: EmotionalElements(
                comfort_symbols=['open door', 'warm tea', 'soft blanket'],
                human_touches=['reading glasses', 'family photos visible'],
                continuity_metaphors=['sunrise', 'new leaves on tree'],
                warmth_markers=['golden light', 'amber tones', 'rose gold'],
                life_elements=['fresh flowers', 'open window', 'morning light']
            ),
            EmotionalTone.TRUSTED_GUIDE: EmotionalElements(
                comfort_symbols=['guiding light', 'steady hand', 'compass'],
                human_touches=['well-worn journal', 'coffee mug', 'personal notes'],
                continuity_metaphors=['lighthouse beam', 'path through garden'],
                warmth_markers=['desk lamp glow', 'leather patina', 'wood grain'],
                life_elements=['plant on desk', 'bookmark in book', 'pen with character']
            ),
            EmotionalTone.FAMILY_HERITAGE: EmotionalElements(
                comfort_symbols=['family tree', 'photo albums', 'heirloom quilt'],
                human_touches=['handwritten recipes', 'children drawings', 'worn edges'],
                continuity_metaphors=['generations in photos', 'oak tree rings', 'river flow'],
                warmth_markers=['fireplace glow', 'sunset colors', 'honey tones'],
                life_elements=['birthday candles', 'garden growth', 'seasonal decorations']
            ),
            EmotionalTone.SECURE_PROTECTION: EmotionalElements(
                comfort_symbols=['umbrella in storm', 'safe harbor', 'strong foundation'],
                human_touches=['family provision', 'protective embrace', 'careful planning'],
                continuity_metaphors=['seeds to garden', 'nest egg', 'bridge to future'],
                warmth_markers=['vault interior warmth', 'gold security', 'trust symbols'],
                life_elements=['growth charts', 'future plans', 'provision markers']
            ),
            EmotionalTone.PEACEFUL_TRANSITION: EmotionalElements(
                comfort_symbols=['gentle sunset', 'calm waters', 'soft twilight'],
                human_touches=['held hands', 'tissue box nearby', 'thoughtful pause'],
                continuity_metaphors=['passing torch', 'tide cycles', 'season change'],
                warmth_markers=['twilight purple', 'star emergence', 'moon glow'],
                life_elements=['candle passing flame', 'letter sealed with care', 'peaceful moment']
            ),
            EmotionalTone.LIVING_CONTINUITY: EmotionalElements(
                comfort_symbols=['eternal flame', 'evergreen', 'circle unbroken'],
                human_touches=['legacy items', 'inherited treasures', 'passed down'],
                continuity_metaphors=['river meeting sea', 'roots and branches', 'echo forward'],
                warmth_markers=['timeless gold', 'heritage bronze', 'lasting warmth'],
                life_elements=['perennial garden', 'family recipes', 'stories retold']
            ),
            EmotionalTone.TECH_BRIDGE: EmotionalElements(
                comfort_symbols=['connection maintained', 'digital embrace', 'cloud safety'],
                human_touches=['video call family', 'digital photos', 'online memories'],
                continuity_metaphors=['wifi as connection', 'cloud as eternal', 'digital legacy'],
                warmth_markers=['screen warmth on wood', 'device glow', 'pixel light'],
                life_elements=['active profiles', 'living documents', 'updated feeds']
            )
        }
    def _initialize_style_library(self) -> Dict[str, StyleElements]:
        return {
            'luxury_base': StyleElements(
                materials=['mahogany', 'leather', 'brass', 'marble', 'silk'],
                lighting=['3-point lighting', 'rim light', 'golden hour', 'lamplight'],
                colors=['deep mahogany', 'warm amber', 'antique gold', 'aged brass'],
                textures=['wood grain', 'leather texture', 'paper fiber', 'fabric weave'],
                objects=['fountain pen', 'pocket watch', 'leather journal', 'crystal glass'],
                composition=['rule of thirds', 'golden ratio', 'layered depth', 'focal hierarchy']
            ),
            'emotional_warmth': StyleElements(
                materials=['worn leather', 'soft fabric', 'weathered wood', 'brushed metal'],
                lighting=['warm glow', 'soft shadows', 'filtered sunlight', 'candle light'],
                colors=['honey gold', 'sage green', 'warm cream', 'sunset orange'],
                textures=['soft edges', 'gentle grain', 'comfortable wear', 'lived patina'],
                objects=['family photos', 'tea cup', 'reading glasses', 'cozy blanket'],
                composition=['intimate framing', 'personal scale', 'approachable angle', 'welcoming space']
            ),
            'professional_trust': StyleElements(
                materials=['polished wood', 'quality paper', 'solid metal', 'clear glass'],
                lighting=['even illumination', 'professional', 'confidence lighting', 'clarity'],
                colors=['navy blue', 'forest green', 'charcoal', 'burgundy'],
                textures=['smooth finish', 'crisp edges', 'professional', 'refined'],
                objects=['legal documents', 'official seals', 'professional tools', 'certificates'],
                composition=['balanced', 'structured', 'organized', 'authoritative']
            )
        }
    def create_prompt(self,
                     title: str,
                     category: str,
                     asset_type: str,
                     tier: str = None,
                     custom_elements: Dict[str, Any] = None) -> str:
        asset_enum = AssetType(asset_type.lower())
        if tier:
            tier_mapping = {
                'tier_1_hub': 'hub',
                'tier_2_section': 'section',
                'tier_3_document': 'document',
                'tier_4_letter': 'letter',
                'tier_5_digital': 'digital'
            }
            tier_value = tier_mapping.get(tier.lower(), tier.lower())
            page_tier = PageTier(tier_value)
        else:
            if 'hub' in title.lower():
                page_tier = PageTier.HUB
            elif 'letter' in title.lower():
                page_tier = PageTier.LETTER
            elif any(term in title.lower() for term in ['google', 'apple', 'facebook', 'digital']):
                page_tier = PageTier.DIGITAL
            elif any(term in title.lower() for term in ['will', 'trust', 'insurance', 'account']):
                page_tier = PageTier.DOCUMENT
            else:
                page_tier = PageTier.SECTION
        section_theme = self.section_themes.get(category.lower(), self.section_themes['family'])
        emotional_tone = section_theme['emotional_tone']
        emotional_elements = self.emotional_mappings[emotional_tone]
        base_descriptions = {
            PageTier.HUB: f"Ultra-luxury command center for {title}, the grand entrance and anchor point for {category} section",
            PageTier.SECTION: f"Premium functional interface for {title} within the {category} domain",
            PageTier.DOCUMENT: f"Professional trustworthy document interface for {title} with security elements",
            PageTier.LETTER: f"Elegant formal correspondence template for {title} with personal touches",
            PageTier.DIGITAL: f"Hybrid luxury-tech interface for {title} blending tradition with modern technology"
        }
        base_description = base_descriptions[page_tier]
        style = self.style_library['luxury_base']
        emotional_style = self.style_library['emotional_warmth']
        prompt_parts = [base_description]
        prompt_parts.append(f"{section_theme['aesthetic']} aesthetic")
        materials = list(set(style.materials + section_theme['materials'][:3]))
        prompt_parts.append(f"featuring {', '.join(materials[:4])}")
        prompt_parts.append(f"illuminated by {section_theme['lighting']}")
        emotional_items = []
        if emotional_elements.comfort_symbols:
            emotional_items.append(emotional_elements.comfort_symbols[0])
        if emotional_elements.human_touches:
            emotional_items.append(emotional_elements.human_touches[0])
        if emotional_elements.continuity_metaphors:
            emotional_items.append(emotional_elements.continuity_metaphors[0])
        if emotional_items:
            prompt_parts.append(f"with {', '.join(emotional_items)} for emotional warmth")
        colors = section_theme['palette']
        prompt_parts.append(f"in {', '.join(colors)} color palette")
        focal_points = {
            'executor': 'scales of justice in warm brass',
            'family': 'multi-generational photo arrangement',
            'financial': 'vault door with family crest',
            'property': 'architectural blueprints with heritage markers',
            'admin': 'control panel with data streams',
            'digital': 'tablet showing family photos',
            'letter': 'wax seal with personal emblem'
        }
        for key, focal in focal_points.items():
            if key in title.lower() or key in category.lower():
                prompt_parts.append(f"centered on {focal}")
                break
        tech_specs = {
            AssetType.ICON: "SVG vector art optimized for 24px-256px display with metallic gradients and dimensional shadows",
            AssetType.COVER: "1500x400px cinematic panoramic composition with 5-7 parallax layers and negative space for content overlay",
            AssetType.LETTER_HEADER: "1920x400px elegant letterhead with watermark patterns at 5% opacity and foil stamp effects",
            AssetType.DATABASE_ICON: "structured data visualization icon with organized grid patterns",
            AssetType.TEXTURE: "512x512px seamless tiling texture with multiple detail levels"
        }
        prompt_parts.append(tech_specs[asset_enum])
        prompt_parts.append("ultra-high-end luxury quality with emotional accessibility")
        complete_prompt = ", ".join(prompt_parts)
        if custom_elements:
            if custom_elements.get('additional_elements'):
                complete_prompt += f", {custom_elements['additional_elements']}"
        return complete_prompt
    def create_prompt_variants(self, base_info: Dict[str, Any], num_variants: int = 3) -> List[str]:
        variants = []
        variant1 = self.create_prompt(
            title=base_info['title'],
            category=base_info['category'],
            asset_type=base_info['asset_type'],
            tier=base_info.get('tier'),
            custom_elements={'additional_elements': 'emphasizing human warmth and personal connection'}
        )
        variants.append(variant1)
        variant2 = self.create_prompt(
            title=base_info['title'],
            category=base_info['category'],
            asset_type=base_info['asset_type'],
            tier=base_info.get('tier'),
            custom_elements={'additional_elements': 'emphasizing ultra-premium materials and sophisticated luxury'}
        )
        variants.append(variant2)
        variant3 = self.create_prompt(
            title=base_info['title'],
            category=base_info['category'],
            asset_type=base_info['asset_type'],
            tier=base_info.get('tier'),
            custom_elements={'additional_elements': 'emphasizing precise composition and visual consistency'}
        )
        variants.append(variant3)
        return variants[:num_variants]
    def save_templates(self, output_file: str = "prompt_templates.json"):
        data = {
            'section_themes': {k: {**v, 'emotional_tone': v['emotional_tone'].value}
                             for k, v in self.section_themes.items()},
            'emotional_mappings': {
                tone.value: {
                    'comfort_symbols': elements.comfort_symbols,
                    'human_touches': elements.human_touches,
                    'continuity_metaphors': elements.continuity_metaphors,
                    'warmth_markers': elements.warmth_markers,
                    'life_elements': elements.life_elements
                }
                for tone, elements in self.emotional_mappings.items()
            },
            'style_library': {
                name: {
                    'materials': style.materials,
                    'lighting': style.lighting,
                    'colors': style.colors,
                    'textures': style.textures,
                    'objects': style.objects,
                    'composition': style.composition
                }
                for name, style in self.style_library.items()
            }
        }
        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)
        return output_file
def test_prompt_templates():
    manager = PromptTemplateManager()
    test_cases = [
        {'title': 'Executor Hub', 'category': 'executor', 'asset_type': 'icon', 'tier': 'hub'},
        {'title': 'Family Messages', 'category': 'family', 'asset_type': 'cover', 'tier': 'section'},
        {'title': 'Last Will', 'category': 'financial', 'asset_type': 'icon', 'tier': 'document'},
        {'title': 'Letter to Spouse', 'category': 'letters', 'asset_type': 'letter_header', 'tier': 'letter'},
        {'title': 'Google Account Recovery', 'category': 'digital', 'asset_type': 'icon', 'tier': 'digital'}
    ]
    print("PROMPT TEMPLATE TESTS")
    print("=" * 80)
    for test in test_cases:
        print(f"\n{test['title']}:")
        print("-" * 40)
        prompt = manager.create_prompt(
            title=test['title'],
            category=test['category'],
            asset_type=test['asset_type'],
            tier=test['tier']
        )
        print(f"Prompt: {prompt[:200]}...")
        variants = manager.create_prompt_variants(test, num_variants=3)
        print(f"Generated {len(variants)} variants")
    output = manager.save_templates()
    print(f"\nTemplates saved to: {output}")
if __name__ == "__main__":
    test_prompt_templates()

================================================================================
FILE: asset_generation/emotional_elements.py
================================================================================

#!/usr/bin/env python3
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import random
from pathlib import Path
import json
class LifeStage(Enum):
    YOUNG_FAMILY = "young_family"
    ESTABLISHED_FAMILY = "established_family"
    EMPTY_NESTERS = "empty_nesters"
    RETIREMENT = "retirement"
    ELDERLY = "elderly"
class EmotionalContext(Enum):
    PROACTIVE_PLANNING = "proactive_planning"
    HEALTH_CONCERN = "health_concern"
    FAMILY_CRISIS = "family_crisis"
    LOSS_PROCESSING = "loss_processing"
    CELEBRATION = "celebration"
class ComfortLevel(Enum):
    ANXIOUS = "anxious"
    CAUTIOUS = "cautious"
    CONFIDENT = "confident"
    EXPERT = "expert"
@dataclass
class EmotionalMarker:
    element: str
    emotional_weight: float
    comfort_factor: float
    universality: float
    description: str
    placement_hints: List[str] = field(default_factory=list)
@dataclass
class ContextualEmotions:
    comfort_symbols: List[EmotionalMarker] = field(default_factory=list)
    human_touches: List[EmotionalMarker] = field(default_factory=list)
    continuity_metaphors: List[EmotionalMarker] = field(default_factory=list)
    warmth_markers: List[EmotionalMarker] = field(default_factory=list)
    life_elements: List[EmotionalMarker] = field(default_factory=list)
    protection_symbols: List[EmotionalMarker] = field(default_factory=list)
class EmotionalElementsManager:
    def __init__(self):
        self.comfort_symbols = self._initialize_comfort_symbols()
        self.human_touches = self._initialize_human_touches()
        self.continuity_metaphors = self._initialize_continuity_metaphors()
        self.warmth_markers = self._initialize_warmth_markers()
        self.life_elements = self._initialize_life_elements()
        self.protection_symbols = self._initialize_protection_symbols()
        self.context_mappings = self._initialize_context_mappings()
        self.cultural_filters = self._initialize_cultural_filters()
    def _initialize_comfort_symbols(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="warm tea service on side table",
                emotional_weight=0.7,
                comfort_factor=0.9,
                universality=0.8,
                description="Universal comfort ritual, suggests patience and care",
                placement_hints=["desk corner", "beside seating", "background element"]
            ),
            EmotionalMarker(
                element="soft reading lamp with amber glow",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.9,
                description="Gentle lighting suggests safety and focus",
                placement_hints=["desk lighting", "background warmth", "focal rim light"]
            ),
            EmotionalMarker(
                element="handknit blanket draped over chair",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.7,
                description="Handmade items suggest care and human touch",
                placement_hints=["chair back", "reading nook", "family area"]
            ),
            EmotionalMarker(
                element="open book with bookmark ribbon",
                emotional_weight=0.5,
                comfort_factor=0.7,
                universality=0.8,
                description="Suggests ongoing learning and thoughtfulness",
                placement_hints=["desk surface", "side table", "background detail"]
            ),
            EmotionalMarker(
                element="fresh flowers in simple vase",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.9,
                description="Life and beauty in planning context",
                placement_hints=["windowsill", "desk corner", "reception area"]
            ),
            EmotionalMarker(
                element="tissue box discretely placed",
                emotional_weight=0.9,
                comfort_factor=0.8,
                universality=0.7,
                description="Acknowledges emotional difficulty, shows preparation",
                placement_hints=["side table", "consultation area", "subtle placement"]
            ),
            EmotionalMarker(
                element="comfortable reading glasses nearby",
                emotional_weight=0.4,
                comfort_factor=0.6,
                universality=0.9,
                description="Practical tool suggests accessibility and care",
                placement_hints=["desk surface", "document area", "personal items"]
            ),
            EmotionalMarker(
                element="handwritten note with caring words",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.8,
                description="Personal touch shows individual attention",
                placement_hints=["document margin", "sticky note", "personal message"]
            )
        ]
    def _initialize_human_touches(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="family photo in wooden frame",
                emotional_weight=0.9,
                comfort_factor=0.8,
                universality=0.9,
                description="Personal connection and motivation for planning",
                placement_hints=["desk corner", "bookshelf", "personal space"]
            ),
            EmotionalMarker(
                element="coffee ring stain on document edge",
                emotional_weight=0.3,
                comfort_factor=0.6,
                universality=0.7,
                description="Shows documents are worked with, not sterile",
                placement_hints=["paper edges", "work surfaces", "lived-in details"]
            ),
            EmotionalMarker(
                element="well-worn leather journal with ribbon",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.8,
                description="Personal record-keeping, thoughtful process",
                placement_hints=["desk surface", "planning materials", "personal tools"]
            ),
            EmotionalMarker(
                element="child's artwork pinned to bulletin board",
                emotional_weight=0.8,
                comfort_factor=0.7,
                universality=0.8,
                description="Family motivation, future generations",
                placement_hints=["background wall", "family area", "motivation reminder"]
            ),
            EmotionalMarker(
                element="handwritten margin notes in documents",
                emotional_weight=0.6,
                comfort_factor=0.7,
                universality=0.8,
                description="Active engagement with planning process",
                placement_hints=["document margins", "planning notes", "personal annotations"]
            ),
            EmotionalMarker(
                element="personal pen with engraved initials",
                emotional_weight=0.5,
                comfort_factor=0.6,
                universality=0.7,
                description="Personal tools show individual touch",
                placement_hints=["document signing", "desk surface", "writing implements"]
            ),
            EmotionalMarker(
                element="wedding ring beside documents",
                emotional_weight=0.9,
                comfort_factor=0.7,
                universality=0.9,
                description="Symbol of commitment and love",
                placement_hints=["document area", "personal items", "commitment reminder"]
            ),
            EmotionalMarker(
                element="travel souvenir used as paperweight",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.6,
                description="Life experiences and memories",
                placement_hints=["desk organizer", "memory keeper", "life celebration"]
            )
        ]
    def _initialize_continuity_metaphors(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="oak tree with visible roots and branches",
                emotional_weight=0.9,
                comfort_factor=0.8,
                universality=0.9,
                description="Generational strength and growth",
                placement_hints=["window view", "artwork", "metaphorical element"]
            ),
            EmotionalMarker(
                element="river flowing toward horizon",
                emotional_weight=0.8,
                comfort_factor=0.8,
                universality=0.8,
                description="Life's journey continuing forward",
                placement_hints=["landscape view", "artwork", "background metaphor"]
            ),
            EmotionalMarker(
                element="lighthouse beam guiding ships",
                emotional_weight=0.7,
                comfort_factor=0.9,
                universality=0.8,
                description="Guidance and protection across time",
                placement_hints=["wall art", "metaphorical guidance", "protection symbol"]
            ),
            EmotionalMarker(
                element="candle flame passing to new candle",
                emotional_weight=0.9,
                comfort_factor=0.8,
                universality=0.9,
                description="Life passing from generation to generation",
                placement_hints=["ceremonial element", "legacy symbol", "gentle metaphor"]
            ),
            EmotionalMarker(
                element="vintage photo album with new photos added",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.9,
                description="Continuing story, adding to family history",
                placement_hints=["family area", "memory collection", "ongoing story"]
            ),
            EmotionalMarker(
                element="garden with perennial flowers",
                emotional_weight=0.7,
                comfort_factor=0.9,
                universality=0.8,
                description="Life renewing itself each season",
                placement_hints=["window view", "outdoor metaphor", "renewal symbol"]
            ),
            EmotionalMarker(
                element="compass pointing toward future",
                emotional_weight=0.6,
                comfort_factor=0.7,
                universality=0.8,
                description="Direction and guidance for what's ahead",
                placement_hints=["navigation symbol", "planning tool", "direction marker"]
            ),
            EmotionalMarker(
                element="sunrise over familiar landscape",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.9,
                description="New day, continuing beauty, hope",
                placement_hints=["window view", "hopeful element", "renewal symbol"]
            )
        ]
    def _initialize_warmth_markers(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="honey-gold lighting",
                emotional_weight=0.7,
                comfort_factor=0.9,
                universality=0.9,
                description="Warm lighting creates emotional safety",
                placement_hints=["general lighting", "accent lighting", "mood setting"]
            ),
            EmotionalMarker(
                element="amber glass lamp shade",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.8,
                description="Filtered warm light, cozy atmosphere",
                placement_hints=["desk lamp", "reading light", "ambient lighting"]
            ),
            EmotionalMarker(
                element="burgundy leather chair with patina",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.7,
                description="Aged materials suggest comfort and reliability",
                placement_hints=["seating area", "consultation space", "comfort zone"]
            ),
            EmotionalMarker(
                element="soft cashmere throw",
                emotional_weight=0.6,
                comfort_factor=0.9,
                universality=0.7,
                description="Luxury comfort, caring attention to comfort",
                placement_hints=["seating area", "comfort element", "luxury touch"]
            ),
            EmotionalMarker(
                element="fireplace with gentle flame",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.8,
                description="Primal comfort, gathering place, warmth",
                placement_hints=["room centerpiece", "gathering area", "warmth source"]
            ),
            EmotionalMarker(
                element="copper accents with warm patina",
                emotional_weight=0.5,
                comfort_factor=0.7,
                universality=0.6,
                description="Warm metals suggest quality and age",
                placement_hints=["hardware", "accent details", "metallic warmth"]
            ),
            EmotionalMarker(
                element="sunset colors reflected in surfaces",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.9,
                description="Natural warm colors, end-of-day peace",
                placement_hints=["color palette", "reflective surfaces", "ambient color"]
            ),
            EmotionalMarker(
                element="wood grain with visible character",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.8,
                description="Natural materials with life history",
                placement_hints=["furniture", "surfaces", "natural elements"]
            )
        ]
    def _initialize_life_elements(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="birthday calendar with family dates",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.9,
                description="Celebrating ongoing life and milestones",
                placement_hints=["wall calendar", "planning area", "family celebrations"]
            ),
            EmotionalMarker(
                element="growth chart marks on doorframe",
                emotional_weight=0.8,
                comfort_factor=0.7,
                universality=0.8,
                description="Children growing, life progressing",
                placement_hints=["doorway", "family area", "growth tracking"]
            ),
            EmotionalMarker(
                element="fresh bread cooling on kitchen counter",
                emotional_weight=0.6,
                comfort_factor=0.9,
                universality=0.8,
                description="Daily life continuing, nourishment, care",
                placement_hints=["kitchen area", "daily life", "nourishment symbol"]
            ),
            EmotionalMarker(
                element="homework papers on desk",
                emotional_weight=0.5,
                comfort_factor=0.6,
                universality=0.8,
                description="Education continuing, future preparation",
                placement_hints=["study area", "family life", "education focus"]
            ),
            EmotionalMarker(
                element="garden tools with soil still on them",
                emotional_weight=0.6,
                comfort_factor=0.7,
                universality=0.7,
                description="Active gardening, nurturing growth",
                placement_hints=["garden area", "growing metaphor", "nurturing activity"]
            ),
            EmotionalMarker(
                element="half-finished puzzle on table",
                emotional_weight=0.5,
                comfort_factor=0.8,
                universality=0.8,
                description="Ongoing projects, patience, family time",
                placement_hints=["family area", "activity table", "ongoing projects"]
            ),
            EmotionalMarker(
                element="pet sleeping peacefully nearby",
                emotional_weight=0.7,
                comfort_factor=0.9,
                universality=0.8,
                description="Companionship, peaceful presence, family",
                placement_hints=["comfort area", "peaceful element", "family member"]
            ),
            EmotionalMarker(
                element="seasonal decorations subtly placed",
                emotional_weight=0.4,
                comfort_factor=0.7,
                universality=0.7,
                description="Life cycles, celebration, ongoing traditions",
                placement_hints=["background details", "seasonal touches", "tradition markers"]
            )
        ]
    def _initialize_protection_symbols(self) -> List[EmotionalMarker]:
        return [
            EmotionalMarker(
                element="umbrella standing ready by door",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.9,
                description="Protection from life's storms, preparedness",
                placement_hints=["entryway", "protection symbol", "preparedness"]
            ),
            EmotionalMarker(
                element="safe harbor artwork on wall",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.8,
                description="Safety, refuge, protection metaphor",
                placement_hints=["wall art", "protection theme", "safety symbol"]
            ),
            EmotionalMarker(
                element="strong foundation visible in architecture",
                emotional_weight=0.8,
                comfort_factor=0.9,
                universality=0.8,
                description="Solid base, reliability, strength",
                placement_hints=["architectural element", "foundation metaphor", "strength"]
            ),
            EmotionalMarker(
                element="watchful guardian statue in garden",
                emotional_weight=0.7,
                comfort_factor=0.7,
                universality=0.6,
                description="Protective presence, watching over family",
                placement_hints=["garden element", "protective figure", "guardian symbol"]
            ),
            EmotionalMarker(
                element="nest with eggs in tree branch",
                emotional_weight=0.8,
                comfort_factor=0.8,
                universality=0.9,
                description="Protection of future generations, nurturing",
                placement_hints=["nature element", "protection metaphor", "future care"]
            ),
            EmotionalMarker(
                element="lighthouse in distance",
                emotional_weight=0.7,
                comfort_factor=0.8,
                universality=0.8,
                description="Guidance through difficult times, beacon",
                placement_hints=["distant view", "guidance symbol", "beacon of hope"]
            ),
            EmotionalMarker(
                element="security system panel (discrete)",
                emotional_weight=0.5,
                comfort_factor=0.7,
                universality=0.7,
                description="Modern protection, practical security",
                placement_hints=["background element", "modern security", "practical protection"]
            ),
            EmotionalMarker(
                element="insurance documents in protective sleeve",
                emotional_weight=0.6,
                comfort_factor=0.8,
                universality=0.8,
                description="Protection through planning, care for documents",
                placement_hints=["document area", "protection through planning", "careful organization"]
            )
        ]
    def _initialize_context_mappings(self) -> Dict[EmotionalContext, ContextualEmotions]:
        return {
            EmotionalContext.PROACTIVE_PLANNING: ContextualEmotions(
                comfort_symbols=self.comfort_symbols[:3],
                human_touches=self.human_touches[:4],
                continuity_metaphors=self.continuity_metaphors[:3],
                warmth_markers=self.warmth_markers[:3],
                life_elements=self.life_elements[:4],
                protection_symbols=self.protection_symbols[:2]
            ),
            EmotionalContext.HEALTH_CONCERN: ContextualEmotions(
                comfort_symbols=self.comfort_symbols[:5],
                human_touches=self.human_touches[:3],
                continuity_metaphors=self.continuity_metaphors[3:6],
                warmth_markers=self.warmth_markers[:4],
                life_elements=self.life_elements[2:5],
                protection_symbols=self.protection_symbols[:4]
            ),
            EmotionalContext.FAMILY_CRISIS: ContextualEmotions(
                comfort_symbols=self.comfort_symbols[1:4],
                human_touches=self.human_touches[2:5],
                continuity_metaphors=self.continuity_metaphors[:4],
                warmth_markers=self.warmth_markers[:5],
                life_elements=self.life_elements[1:4],
                protection_symbols=self.protection_symbols[2:5]
            ),
            EmotionalContext.LOSS_PROCESSING: ContextualEmotions(
                comfort_symbols=self.comfort_symbols[:6],
                human_touches=self.human_touches[1:5],
                continuity_metaphors=self.continuity_metaphors[2:7],
                warmth_markers=self.warmth_markers[:6],
                life_elements=self.life_elements[:3],
                protection_symbols=self.protection_symbols[1:4]
            ),
            EmotionalContext.CELEBRATION: ContextualEmotions(
                comfort_symbols=self.comfort_symbols[2:5],
                human_touches=self.human_touches[:5],
                continuity_metaphors=self.continuity_metaphors[:4],
                warmth_markers=self.warmth_markers[1:5],
                life_elements=self.life_elements[:6],
                protection_symbols=self.protection_symbols[:3]
            )
        }
    def _initialize_cultural_filters(self) -> Dict[str, List[str]]:
        return {
            'universal_safe': [
                'natural elements', 'family photos', 'books', 'tea/coffee',
                'flowers', 'warm lighting', 'comfortable furniture',
                'handwritten notes', 'reading glasses'
            ],
            'avoid_assumptions': [
                'specific religious symbols', 'gender role assumptions',
                'family structure assumptions', 'cultural celebrations',
                'specific foods', 'particular traditions'
            ],
            'inclusive_elements': [
                'diverse family configurations', 'various ages together',
                'different ability levels', 'multiple generations',
                'various cultural backgrounds subtly included'
            ]
        }
    def get_contextual_elements(self,
                              emotional_context: EmotionalContext,
                              comfort_level: ComfortLevel,
                              num_elements: int = 3) -> Dict[str, List[str]]:
        context_emotions = self.context_mappings[emotional_context]
        intensity_multiplier = {
            ComfortLevel.ANXIOUS: 1.2,
            ComfortLevel.CAUTIOUS: 1.0,
            ComfortLevel.CONFIDENT: 0.8,
            ComfortLevel.EXPERT: 0.6
        }
        multiplier = intensity_multiplier[comfort_level]
        adjusted_num = max(1, int(num_elements * multiplier))
        selected_elements = {
            'comfort_symbols': [
                elem.element for elem in
                sorted(context_emotions.comfort_symbols,
                      key=lambda x: x.comfort_factor, reverse=True)[:adjusted_num]
            ],
            'human_touches': [
                elem.element for elem in
                sorted(context_emotions.human_touches,
                      key=lambda x: x.comfort_factor, reverse=True)[:adjusted_num]
            ],
            'continuity_metaphors': [
                elem.element for elem in
                sorted(context_emotions.continuity_metaphors,
                      key=lambda x: x.emotional_weight, reverse=True)[:adjusted_num]
            ],
            'warmth_markers': [
                elem.element for elem in
                sorted(context_emotions.warmth_markers,
                      key=lambda x: x.comfort_factor, reverse=True)[:adjusted_num]
            ],
            'life_elements': [
                elem.element for elem in
                sorted(context_emotions.life_elements,
                      key=lambda x: x.universality, reverse=True)[:adjusted_num]
            ],
            'protection_symbols': [
                elem.element for elem in
                sorted(context_emotions.protection_symbols,
                      key=lambda x: (x.comfort_factor + x.universality)/2, reverse=True)[:adjusted_num]
            ]
        }
        return selected_elements
    def generate_emotional_prompt_additions(self,
                                          base_prompt: str,
                                          emotional_context: EmotionalContext = EmotionalContext.PROACTIVE_PLANNING,
                                          comfort_level: ComfortLevel = ComfortLevel.CAUTIOUS,
                                          emphasis: str = "balanced") -> str:
        elements = self.get_contextual_elements(emotional_context, comfort_level, num_elements=2)
        if emphasis == "warmth":
            additions = elements['warmth_markers'] + elements['comfort_symbols']
        elif emphasis == "continuity":
            additions = elements['continuity_metaphors'] + elements['life_elements']
        elif emphasis == "protection":
            additions = elements['protection_symbols'] + elements['comfort_symbols']
        elif emphasis == "human":
            additions = elements['human_touches'] + elements['life_elements']
        else:
            additions = (
                elements['comfort_symbols'][:1] +
                elements['human_touches'][:1] +
                elements['warmth_markers'][:1]
            )
        additions = [add for add in additions if add.strip()]
        if additions:
            emotional_addition = f", featuring {', '.join(additions)} for emotional warmth and human connection"
            return base_prompt + emotional_addition
        return base_prompt
    def save_emotional_library(self, output_file: str = "emotional_elements.json"):
        def serialize_marker(marker: EmotionalMarker) -> Dict[str, Any]:
            return {
                'element': marker.element,
                'emotional_weight': marker.emotional_weight,
                'comfort_factor': marker.comfort_factor,
                'universality': marker.universality,
                'description': marker.description,
                'placement_hints': marker.placement_hints
            }
        data = {
            'comfort_symbols': [serialize_marker(m) for m in self.comfort_symbols],
            'human_touches': [serialize_marker(m) for m in self.human_touches],
            'continuity_metaphors': [serialize_marker(m) for m in self.continuity_metaphors],
            'warmth_markers': [serialize_marker(m) for m in self.warmth_markers],
            'life_elements': [serialize_marker(m) for m in self.life_elements],
            'protection_symbols': [serialize_marker(m) for m in self.protection_symbols],
            'cultural_filters': self.cultural_filters
        }
        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)
        return output_file
def test_emotional_elements():
    manager = EmotionalElementsManager()
    print("EMOTIONAL ELEMENTS SYSTEM TEST")
    print("=" * 80)
    test_contexts = [
        (EmotionalContext.PROACTIVE_PLANNING, ComfortLevel.CONFIDENT),
        (EmotionalContext.HEALTH_CONCERN, ComfortLevel.ANXIOUS),
        (EmotionalContext.LOSS_PROCESSING, ComfortLevel.CAUTIOUS),
        (EmotionalContext.CELEBRATION, ComfortLevel.CONFIDENT)
    ]
    for context, comfort in test_contexts:
        print(f"\nContext: {context.value}, Comfort Level: {comfort.value}")
        print("-" * 60)
        elements = manager.get_contextual_elements(context, comfort, num_elements=3)
        for category, items in elements.items():
            if items:
                print(f"{category}: {', '.join(items[:2])}")
        base_prompt = "Ultra-luxury icon for Estate Planning Dashboard: mahogany desk with legal documents"
        enhanced = manager.generate_emotional_prompt_additions(
            base_prompt, context, comfort, emphasis="warmth"
        )
        print(f"\nEnhanced prompt: {enhanced[:100]}...")
    output = manager.save_emotional_library()
    print(f"\nEmotional library saved to: {output}")
if __name__ == "__main__":
    test_emotional_elements()

================================================================================
FILE: asset_generation/visual_hierarchy.py
================================================================================

#!/usr/bin/env python3
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import re
from pathlib import Path
import json
class VisualTier(Enum):
    TIER_1_HUB = "tier_1_hub"
    TIER_2_SECTION = "tier_2_section"
    TIER_3_DOCUMENT = "tier_3_document"
    TIER_4_LETTER = "tier_4_letter"
    TIER_5_DIGITAL = "tier_5_digital"
class SectionType(Enum):
    ADMIN = "admin"
    EXECUTOR = "executor"
    FAMILY = "family"
    FINANCIAL = "financial"
    PROPERTY = "property"
    DIGITAL = "digital"
    LETTERS = "letters"
@dataclass
class ComplexityProfile:
    layer_count: int
    detail_density: float
    metallic_intensity: float
    texture_layers: int
    focal_elements: int
    lighting_complexity: str
@dataclass
class SectionAesthetic:
    name: str
    theme_description: str
    color_palette: List[str]
    primary_materials: List[str]
    secondary_materials: List[str]
    lighting_style: str
    architectural_elements: List[str]
    emotional_tone: str
    luxury_markers: List[str]
@dataclass
class HierarchyRule:
    tier: VisualTier
    complexity_profile: ComplexityProfile
    inheritance_rules: Dict[str, Any]
    differentiation_requirements: List[str]
class VisualHierarchyManager:
    def __init__(self):
        self.tier_profiles = self._initialize_tier_profiles()
        self.section_aesthetics = self._initialize_section_aesthetics()
        self.hierarchy_rules = self._initialize_hierarchy_rules()
        self.hub_signatures = self._initialize_hub_signatures()
    def _initialize_tier_profiles(self) -> Dict[VisualTier, ComplexityProfile]:
        return {
            VisualTier.TIER_1_HUB: ComplexityProfile(
                layer_count=7,
                detail_density=1.0,
                metallic_intensity=1.0,
                texture_layers=5,
                focal_elements=3,
                lighting_complexity="cinematic_multi_source"
            ),
            VisualTier.TIER_2_SECTION: ComplexityProfile(
                layer_count=5,
                detail_density=0.8,
                metallic_intensity=0.8,
                texture_layers=4,
                focal_elements=2,
                lighting_complexity="professional_two_source"
            ),
            VisualTier.TIER_3_DOCUMENT: ComplexityProfile(
                layer_count=4,
                detail_density=0.7,
                metallic_intensity=0.6,
                texture_layers=3,
                focal_elements=1,
                lighting_complexity="trusted_single_source"
            ),
            VisualTier.TIER_4_LETTER: ComplexityProfile(
                layer_count=3,
                detail_density=0.6,
                metallic_intensity=0.9,
                texture_layers=4,
                focal_elements=1,
                lighting_complexity="elegant_ambient"
            ),
            VisualTier.TIER_5_DIGITAL: ComplexityProfile(
                layer_count=5,
                detail_density=0.8,
                metallic_intensity=0.7,
                texture_layers=3,
                focal_elements=2,
                lighting_complexity="hybrid_tech_warm"
            )
        }
    def _initialize_section_aesthetics(self) -> Dict[SectionType, SectionAesthetic]:
        return {
            SectionType.ADMIN: SectionAesthetic(
                name="Executive Command Center",
                theme_description="Mission Control meets Executive Boardroom",
                color_palette=["charcoal (
                primary_materials=["brushed steel", "smoked glass", "carbon fiber"],
                secondary_materials=["matte black surfaces", "LED strips", "holographic displays"],
                lighting_style="cool focused LED with blue accent strips",
                architectural_elements=["geometric panels", "data visualization screens", "control interfaces"],
                emotional_tone="confident_authority",
                luxury_markers=["precision engineering", "aerospace materials", "cutting-edge technology"]
            ),
            SectionType.EXECUTOR: SectionAesthetic(
                name="Private Law Library",
                theme_description="Law Library meets Private Study",
                color_palette=["deep mahogany (
                primary_materials=["rich mahogany wood", "leather-bound books", "aged brass fixtures"],
                secondary_materials=["parchment paper", "velvet curtains", "bronze hardware"],
                lighting_style="warm lamplight filtering through amber glass shades",
                architectural_elements=["wood paneling", "built-in bookshelves", "coffered ceilings"],
                emotional_tone="trusted_wisdom",
                luxury_markers=["hand-carved details", "leather binding", "traditional craftsmanship"]
            ),
            SectionType.FAMILY: SectionAesthetic(
                name="Heritage Estate Library",
                theme_description="Heritage Estate meets Memory Lane",
                color_palette=["warm oak (
                primary_materials=["heirloom oak wood", "vintage fabrics", "family silver"],
                secondary_materials=["photo albums", "handwritten letters", "quilted textiles"],
                lighting_style="soft golden hour sunlight through lace curtains",
                architectural_elements=["bay windows", "reading nooks", "display shelves"],
                emotional_tone="loving_heritage",
                luxury_markers=["generational pieces", "handmade textiles", "family treasures"]
            ),
            SectionType.FINANCIAL: SectionAesthetic(
                name="Private Banking Vault",
                theme_description="Private Bank meets Vault Room",
                color_palette=["navy blue (
                primary_materials=["Italian marble", "polished brass", "security glass"],
                secondary_materials=["vault steel", "leather portfolios", "crystal accents"],
                lighting_style="precise clean confidence lighting with gold highlights",
                architectural_elements=["marble columns", "vault doors", "security panels"],
                emotional_tone="secure_prosperity",
                luxury_markers=["bank-grade materials", "precision engineering", "timeless elegance"]
            ),
            SectionType.PROPERTY: SectionAesthetic(
                name="Estate Architect's Office",
                theme_description="Architect's Office meets Estate Grounds",
                color_palette=["blueprint blue (
                primary_materials=["drafting paper", "copper accents", "natural stone"],
                secondary_materials=["surveyor tools", "architectural models", "estate maps"],
                lighting_style="natural daylight precision with architectural task lighting",
                architectural_elements=["drafting tables", "blueprint storage", "scale models"],
                emotional_tone="methodical_legacy",
                luxury_markers=["custom architectural details", "precision instruments", "estate craftsmanship"]
            ),
            SectionType.DIGITAL: SectionAesthetic(
                name="Tech-Heritage Bridge",
                theme_description="Traditional Luxury meets Modern Technology",
                color_palette=["mahogany with blue glow (
                primary_materials=["mahogany desk surfaces", "brushed aluminum", "tempered glass"],
                secondary_materials=["fiber optic cables", "LED displays", "wireless chargers"],
                lighting_style="ambient screen glow reflecting on traditional surfaces",
                architectural_elements=["hidden cable management", "integrated displays", "wireless zones"],
                emotional_tone="connected_continuity",
                luxury_markers=["seamless integration", "premium tech materials", "invisible complexity"]
            ),
            SectionType.LETTERS: SectionAesthetic(
                name="Formal Correspondence Suite",
                theme_description="Formal Correspondence meets Personal Touch",
                color_palette=["cream parchment (
                primary_materials=["quality writing paper", "fountain pen metals", "sealing wax"],
                secondary_materials=["ribbon ties", "letter openers", "blotting paper"],
                lighting_style="soft desk lamp illumination on writing surface",
                architectural_elements=["writing desk", "letter storage", "correspondence tools"],
                emotional_tone="formal_intimacy",
                luxury_markers=["calligraphy quality", "premium stationery", "personal seals"]
            )
        }
    def _initialize_hierarchy_rules(self) -> Dict[VisualTier, HierarchyRule]:
        return {
            VisualTier.TIER_1_HUB: HierarchyRule(
                tier=VisualTier.TIER_1_HUB,
                complexity_profile=self.tier_profiles[VisualTier.TIER_1_HUB],
                inheritance_rules={
                    "establishes_section_dna": True,
                    "signature_elements_required": True,
                    "maximum_visual_impact": True,
                    "unique_focal_points": 3,
                    "metallic_gradients": "three_tier_gold_to_bronze"
                },
                differentiation_requirements=[
                    "unique_architectural_elements",
                    "signature_lighting_approach",
                    "distinctive_metallic_accents",
                    "exclusive_texture_combinations"
                ]
            ),
            VisualTier.TIER_2_SECTION: HierarchyRule(
                tier=VisualTier.TIER_2_SECTION,
                complexity_profile=self.tier_profiles[VisualTier.TIER_2_SECTION],
                inheritance_rules={
                    "inherits_hub_base_layer": True,
                    "adds_functional_overlay": True,
                    "maintains_section_temperature": True,
                    "unique_focal_points": 2,
                    "metallic_gradients": "two_tier_primary_to_secondary"
                },
                differentiation_requirements=[
                    "functional_purpose_clear",
                    "hub_relationship_visible",
                    "individual_identity_maintained"
                ]
            ),
            VisualTier.TIER_3_DOCUMENT: HierarchyRule(
                tier=VisualTier.TIER_3_DOCUMENT,
                complexity_profile=self.tier_profiles[VisualTier.TIER_3_DOCUMENT],
                inheritance_rules={
                    "professional_trust_priority": True,
                    "security_elements_required": True,
                    "watermark_patterns": True,
                    "unique_focal_points": 1,
                    "metallic_gradients": "security_seal_emphasis"
                },
                differentiation_requirements=[
                    "document_type_identification",
                    "security_visual_markers",
                    "trustworthiness_indicators"
                ]
            ),
            VisualTier.TIER_4_LETTER: HierarchyRule(
                tier=VisualTier.TIER_4_LETTER,
                complexity_profile=self.tier_profiles[VisualTier.TIER_4_LETTER],
                inheritance_rules={
                    "letterhead_consistency": True,
                    "formal_elegance_required": True,
                    "wax_seal_elements": True,
                    "unique_focal_points": 1,
                    "metallic_gradients": "wax_seal_to_paper"
                },
                differentiation_requirements=[
                    "letter_type_distinction",
                    "formality_level_appropriate",
                    "personal_touch_balance"
                ]
            ),
            VisualTier.TIER_5_DIGITAL: HierarchyRule(
                tier=VisualTier.TIER_5_DIGITAL,
                complexity_profile=self.tier_profiles[VisualTier.TIER_5_DIGITAL],
                inheritance_rules={
                    "hybrid_aesthetic_required": True,
                    "platform_integration_subtle": True,
                    "tech_warmth_balance": True,
                    "unique_focal_points": 2,
                    "metallic_gradients": "circuit_to_traditional"
                },
                differentiation_requirements=[
                    "platform_specific_hints",
                    "technology_level_appropriate",
                    "traditional_luxury_maintained"
                ]
            )
        }
    def _initialize_hub_signatures(self) -> Dict[SectionType, Dict[str, Any]]:
        return {
            SectionType.EXECUTOR: {
                "signature_architectural": "coffered ceiling with warm wood tones",
                "signature_lighting": "banker's lamp with amber glass shade",
                "signature_metallic": "aged brass with patina highlights",
                "signature_texture": "leather book spines with gold lettering",
                "signature_focal": "scales of justice in warm bronze"
            },
            SectionType.FAMILY: {
                "signature_architectural": "bay window with window seat and cushions",
                "signature_lighting": "golden hour sunlight through lace curtains",
                "signature_metallic": "antique gold picture frame accents",
                "signature_texture": "heirloom quilt patterns in fabric textures",
                "signature_focal": "multi-generational family photo arrangement"
            },
            SectionType.FINANCIAL: {
                "signature_architectural": "marble columns with brass capital details",
                "signature_lighting": "precise LED strips with warm gold accents",
                "signature_metallic": "champagne gold vault hardware",
                "signature_texture": "Italian marble veining with security glass",
                "signature_focal": "vault door with family crest medallion"
            },
            SectionType.ADMIN: {
                "signature_architectural": "geometric control panels with data displays",
                "signature_lighting": "cool LED strips with electric blue accents",
                "signature_metallic": "brushed platinum with precision edges",
                "signature_texture": "carbon fiber weave with holographic elements",
                "signature_focal": "multi-screen command interface"
            }
        }
    def determine_visual_tier(self, title: str, category: str, asset_type: str) -> VisualTier:
        title_lower = title.lower()
        category_lower = category.lower()
        hub_indicators = ['hub', 'dashboard', 'main', 'home', 'center', 'cockpit']
        if any(indicator in title_lower for indicator in hub_indicators):
            return VisualTier.TIER_1_HUB
        letter_indicators = ['letter', 'message', 'note', 'correspondence']
        if any(indicator in title_lower for indicator in letter_indicators) or 'letter' in category_lower:
            return VisualTier.TIER_4_LETTER
        digital_indicators = ['google', 'apple', 'facebook', 'digital', 'online', 'cloud', 'account', 'social']
        if any(indicator in title_lower for indicator in digital_indicators):
            return VisualTier.TIER_5_DIGITAL
        document_indicators = ['will', 'trust', 'insurance', 'policy', 'account', 'certificate', 'deed', 'contract']
        if any(indicator in title_lower for indicator in document_indicators):
            return VisualTier.TIER_3_DOCUMENT
        return VisualTier.TIER_2_SECTION
    def determine_section_type(self, title: str, category: str) -> SectionType:
        title_lower = title.lower()
        category_lower = category.lower()
        admin_indicators = ['admin', 'builder', 'setup', 'config', 'rollout', 'diagnostic']
        if any(indicator in title_lower or indicator in category_lower for indicator in admin_indicators):
            return SectionType.ADMIN
        executor_indicators = ['executor', 'estate', 'legal', 'probate', 'will', 'trust']
        if any(indicator in title_lower or indicator in category_lower for indicator in executor_indicators):
            return SectionType.EXECUTOR
        family_indicators = ['family', 'spouse', 'children', 'beneficiary', 'heir', 'message', 'keepsake']
        if any(indicator in title_lower or indicator in category_lower for indicator in family_indicators):
            return SectionType.FAMILY
        financial_indicators = ['financial', 'bank', 'account', 'insurance', 'investment', 'asset']
        if any(indicator in title_lower or indicator in category_lower for indicator in financial_indicators):
            return SectionType.FINANCIAL
        property_indicators = ['property', 'real estate', 'home', 'land', 'building', 'vehicle']
        if any(indicator in title_lower or indicator in category_lower for indicator in property_indicators):
            return SectionType.PROPERTY
        digital_indicators = ['digital', 'online', 'google', 'apple', 'facebook', 'cloud', 'social']
        if any(indicator in title_lower or indicator in category_lower for indicator in digital_indicators):
            return SectionType.DIGITAL
        if 'letter' in title_lower or 'letter' in category_lower:
            return SectionType.LETTERS
        return SectionType.FAMILY
    def generate_tier_specific_elements(self,
                                      visual_tier: VisualTier,
                                      section_type: SectionType) -> Dict[str, Any]:
        tier_profile = self.tier_profiles[visual_tier]
        section_aesthetic = self.section_aesthetics[section_type]
        hierarchy_rule = self.hierarchy_rules[visual_tier]
        elements = {
            "layer_count": tier_profile.layer_count,
            "detail_density": tier_profile.detail_density,
            "metallic_intensity": tier_profile.metallic_intensity,
            "texture_layers": tier_profile.texture_layers,
            "focal_elements": tier_profile.focal_elements,
            "lighting_complexity": tier_profile.lighting_complexity,
            "color_palette": section_aesthetic.color_palette,
            "primary_materials": section_aesthetic.primary_materials,
            "lighting_style": section_aesthetic.lighting_style,
            "architectural_elements": section_aesthetic.architectural_elements,
            "emotional_tone": section_aesthetic.emotional_tone,
            "inheritance_rules": hierarchy_rule.inheritance_rules,
            "differentiation_requirements": hierarchy_rule.differentiation_requirements
        }
        if visual_tier == VisualTier.TIER_1_HUB and section_type in self.hub_signatures:
            elements["hub_signatures"] = self.hub_signatures[section_type]
        return elements
    def create_hierarchical_prompt_elements(self,
                                          title: str,
                                          category: str,
                                          asset_type: str) -> Dict[str, Any]:
        visual_tier = self.determine_visual_tier(title, category, asset_type)
        section_type = self.determine_section_type(title, category)
        tier_elements = self.generate_tier_specific_elements(visual_tier, section_type)
        tier_descriptions = {
            VisualTier.TIER_1_HUB: f"Ultra-luxury command center establishing the visual DNA for {section_type.value} section",
            VisualTier.TIER_2_SECTION: f"Premium functional interface inheriting {section_type.value} hub aesthetics with unique overlay",
            VisualTier.TIER_3_DOCUMENT: f"Professional trustworthy document with security elements and {section_type.value} undertones",
            VisualTier.TIER_4_LETTER: f"Elegant formal correspondence template with {section_type.value} personalization",
            VisualTier.TIER_5_DIGITAL: f"Hybrid luxury-tech interface blending {section_type.value} tradition with modern technology"
        }
        consistency_markers = []
        if visual_tier != VisualTier.TIER_1_HUB:
            consistency_markers.append(f"maintaining {section_type.value} section visual DNA")
        if visual_tier == VisualTier.TIER_2_SECTION:
            consistency_markers.append("inheriting hub's base aesthetic with functional differentiation")
        return {
            "visual_tier": visual_tier.value,
            "section_type": section_type.value,
            "tier_description": tier_descriptions[visual_tier],
            "consistency_markers": consistency_markers,
            "tier_elements": tier_elements,
            "hierarchy_level": f"Tier {visual_tier.value.split('_')[1]} of 5"
        }
    def validate_visual_consistency(self, pages: List[Dict[str, Any]]) -> Dict[str, Any]:
        validation_results = {
            "hub_pages": [],
            "section_consistency": {},
            "tier_distribution": {},
            "potential_conflicts": [],
            "recommendations": []
        }
        for page in pages:
            hierarchy_info = self.create_hierarchical_prompt_elements(
                page.get('title', ''),
                page.get('category', ''),
                page.get('asset_type', 'icon')
            )
            page['hierarchy_info'] = hierarchy_info
            if hierarchy_info['visual_tier'] == 'tier_1_hub':
                validation_results['hub_pages'].append(page)
            section = hierarchy_info['section_type']
            if section not in validation_results['section_consistency']:
                validation_results['section_consistency'][section] = []
            validation_results['section_consistency'][section].append(page)
            tier = hierarchy_info['visual_tier']
            if tier not in validation_results['tier_distribution']:
                validation_results['tier_distribution'][tier] = 0
            validation_results['tier_distribution'][tier] += 1
        for section, section_pages in validation_results['section_consistency'].items():
            hub_count = sum(1 for p in section_pages if p['hierarchy_info']['visual_tier'] == 'tier_1_hub')
            if hub_count == 0:
                validation_results['potential_conflicts'].append(
                    f"Section '{section}' has no hub page to establish visual DNA"
                )
                validation_results['recommendations'].append(
                    f"Consider designating a primary page in '{section}' section as a hub"
                )
            elif hub_count > 1:
                validation_results['potential_conflicts'].append(
                    f"Section '{section}' has {hub_count} hub pages - may create visual confusion"
                )
        return validation_results
    def save_hierarchy_definitions(self, output_file: str = "visual_hierarchy.json"):
        data = {
            'tier_profiles': {
                tier.value: {
                    'layer_count': profile.layer_count,
                    'detail_density': profile.detail_density,
                    'metallic_intensity': profile.metallic_intensity,
                    'texture_layers': profile.texture_layers,
                    'focal_elements': profile.focal_elements,
                    'lighting_complexity': profile.lighting_complexity
                }
                for tier, profile in self.tier_profiles.items()
            },
            'section_aesthetics': {
                section.value: {
                    'name': aesthetic.name,
                    'theme_description': aesthetic.theme_description,
                    'color_palette': aesthetic.color_palette,
                    'primary_materials': aesthetic.primary_materials,
                    'secondary_materials': aesthetic.secondary_materials,
                    'lighting_style': aesthetic.lighting_style,
                    'architectural_elements': aesthetic.architectural_elements,
                    'emotional_tone': aesthetic.emotional_tone,
                    'luxury_markers': aesthetic.luxury_markers
                }
                for section, aesthetic in self.section_aesthetics.items()
            },
            'hub_signatures': {
                section.value: signatures
                for section, signatures in self.hub_signatures.items()
            }
        }
        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)
        return output_file
def test_visual_hierarchy():
    manager = VisualHierarchyManager()
    print("VISUAL HIERARCHY SYSTEM TEST")
    print("=" * 80)
    test_pages = [
        {'title': 'Executor Hub', 'category': 'executor', 'asset_type': 'icon'},
        {'title': 'Family Hub', 'category': 'family', 'asset_type': 'cover'},
        {'title': 'Bank Account Access', 'category': 'executor', 'asset_type': 'icon'},
        {'title': 'Messages for Family', 'category': 'family', 'asset_type': 'cover'},
        {'title': 'Last Will and Testament', 'category': 'financial', 'asset_type': 'icon'},
        {'title': 'Letter to Spouse', 'category': 'letters', 'asset_type': 'letter_header'},
        {'title': 'Google Account Recovery', 'category': 'digital', 'asset_type': 'icon'},
        {'title': 'Admin Rollout Setup', 'category': 'admin', 'asset_type': 'icon'}
    ]
    for page in test_pages:
        print(f"\n{page['title']}:")
        print("-" * 40)
        hierarchy = manager.create_hierarchical_prompt_elements(
            page['title'], page['category'], page['asset_type']
        )
        print(f"Visual Tier: {hierarchy['visual_tier']}")
        print(f"Section Type: {hierarchy['section_type']}")
        print(f"Description: {hierarchy['tier_description'][:80]}...")
        tier_elements = hierarchy['tier_elements']
        print(f"Layers: {tier_elements['layer_count']}, Focal Points: {tier_elements['focal_elements']}")
        print(f"Colors: {', '.join(tier_elements['color_palette'][:2])}...")
    print(f"\n{'VALIDATION RESULTS':=^80}")
    validation = manager.validate_visual_consistency(test_pages)
    print(f"Hub Pages Found: {len(validation['hub_pages'])}")
    for hub in validation['hub_pages']:
        print(f"  • {hub['title']} ({hub['hierarchy_info']['section_type']})")
    print(f"\nTier Distribution:")
    for tier, count in validation['tier_distribution'].items():
        print(f"  • {tier}: {count} pages")
    if validation['potential_conflicts']:
        print(f"\nPotential Issues:")
        for issue in validation['potential_conflicts']:
            print(f"  ⚠️ {issue}")
    if validation['recommendations']:
        print(f"\nRecommendations:")
        for rec in validation['recommendations']:
            print(f"  💡 {rec}")
    output = manager.save_hierarchy_definitions()
    print(f"\nHierarchy definitions saved to: {output}")
if __name__ == "__main__":
    test_visual_hierarchy()

================================================================================
FILE: asset_generation/sample_generator.py
================================================================================

#!/usr/bin/env python3
import os
import json
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from pathlib import Path
import random
from openrouter_orchestrator import OpenRouterOrchestrator, PromptCompetition
from sync_yaml_comprehensive import YAMLSyncComprehensive
from visual_hierarchy import VisualTier, SectionType
from emotional_elements import EmotionalContext, ComfortLevel
@dataclass
class SampleCategory:
    name: str
    visual_tier: VisualTier
    section_theme: SectionType
    emotional_context: EmotionalContext
    comfort_level: ComfortLevel
    sample_titles: List[str]
@dataclass
class SampleMatrix:
    categories: List[SampleCategory]
    asset_types: List[str]
    total_samples: int
    generation_timestamp: str
@dataclass
class GeneratedSample:
    category: str
    asset_type: str
    title: str
    base_prompt: str
    enhanced_prompts: List[str]
    emotional_markers: List[str]
    luxury_indicators: List[str]
    visual_tier: str
    section_theme: str
    quality_scores: Optional[Dict[str, float]] = None
    generation_time: Optional[float] = None
class SampleGenerator:
    def __init__(self, yaml_dir: str = "../split_yaml"):
        self.yaml_system = YAMLSyncComprehensive(yaml_dir)
        self.orchestrator = OpenRouterOrchestrator()
        self.logger = self._setup_logger()
        self.sample_categories = self._define_sample_categories()
        self.asset_types = ['icon', 'cover', 'letter_header']
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger('SampleGenerator')
        logger.setLevel(logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        fh = logging.FileHandler('sample_generation.log')
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        return logger
    def _define_sample_categories(self) -> List[SampleCategory]:
        return [
            SampleCategory(
                name="Preparation Hub",
                visual_tier=VisualTier.TIER_1_HUB,
                section_theme=SectionType.ADMIN,
                emotional_context=EmotionalContext.PROACTIVE_PLANNING,
                comfort_level=ComfortLevel.CONFIDENT,
                sample_titles=["Preparation Hub", "Getting Started Guide", "Estate Planning Overview"]
            ),
            SampleCategory(
                name="Executor Hub",
                visual_tier=VisualTier.TIER_1_HUB,
                section_theme=SectionType.EXECUTOR,
                emotional_context=EmotionalContext.PROACTIVE_PLANNING,
                comfort_level=ComfortLevel.EXPERT,
                sample_titles=["Executor Hub", "Executor Responsibilities", "Estate Administration"]
            ),
            SampleCategory(
                name="Family Hub",
                visual_tier=VisualTier.TIER_1_HUB,
                section_theme=SectionType.FAMILY,
                emotional_context=EmotionalContext.CELEBRATION,
                comfort_level=ComfortLevel.CAUTIOUS,
                sample_titles=["Family Hub", "Messages for Loved Ones", "Legacy Preservation"]
            ),
            SampleCategory(
                name="Financial Accounts",
                visual_tier=VisualTier.TIER_2_SECTION,
                section_theme=SectionType.FINANCIAL,
                emotional_context=EmotionalContext.PROACTIVE_PLANNING,
                comfort_level=ComfortLevel.EXPERT,
                sample_titles=["Bank Accounts", "Investment Portfolio", "Financial Assets"]
            ),
            SampleCategory(
                name="Legal Documents",
                visual_tier=VisualTier.TIER_3_DOCUMENT,
                section_theme=SectionType.EXECUTOR,
                emotional_context=EmotionalContext.PROACTIVE_PLANNING,
                comfort_level=ComfortLevel.EXPERT,
                sample_titles=["Last Will & Testament", "Power of Attorney", "Trust Documents"]
            ),
            SampleCategory(
                name="Digital Legacy",
                visual_tier=VisualTier.TIER_2_SECTION,
                section_theme=SectionType.DIGITAL,
                emotional_context=EmotionalContext.PROACTIVE_PLANNING,
                comfort_level=ComfortLevel.CONFIDENT,
                sample_titles=["Google Account Access", "Social Media Accounts", "Digital Assets"]
            ),
            SampleCategory(
                name="Funeral Planning",
                visual_tier=VisualTier.TIER_2_SECTION,
                section_theme=SectionType.FAMILY,
                emotional_context=EmotionalContext.LOSS_PROCESSING,
                comfort_level=ComfortLevel.CAUTIOUS,
                sample_titles=["Funeral Preferences", "Memorial Service", "Final Arrangements"]
            ),
            SampleCategory(
                name="Medical Directives",
                visual_tier=VisualTier.TIER_3_DOCUMENT,
                section_theme=SectionType.FAMILY,
                emotional_context=EmotionalContext.HEALTH_CONCERN,
                comfort_level=ComfortLevel.CONFIDENT,
                sample_titles=["Living Will", "Healthcare Proxy", "Medical Preferences"]
            ),
            SampleCategory(
                name="Letter Templates",
                visual_tier=VisualTier.TIER_4_LETTER,
                section_theme=SectionType.FAMILY,
                emotional_context=EmotionalContext.CELEBRATION,
                comfort_level=ComfortLevel.ANXIOUS,
                sample_titles=["Letter to Spouse", "Letter to Children", "Final Messages"]
            )
        ]
    async def generate_sample_matrix(self) -> SampleMatrix:
        self.logger.info("Generating 3x3 sample matrix for Estate Planning Concierge v4.0")
        samples = []
        total_start_time = datetime.now()
        for category in self.sample_categories:
            for asset_type in self.asset_types:
                title = random.choice(category.sample_titles)
                sample = await self._generate_single_sample(category, asset_type, title)
                samples.append(sample)
                self.logger.info(f"Generated sample: {category.name} - {asset_type} - {title}")
                await asyncio.sleep(0.5)
        total_time = (datetime.now() - total_start_time).total_seconds()
        matrix = SampleMatrix(
            categories=self.sample_categories,
            asset_types=self.asset_types,
            total_samples=len(samples),
            generation_timestamp=datetime.now().isoformat()
        )
        self.logger.info(f"Sample matrix generation complete: {len(samples)} samples in {total_time:.1f}s")
        return matrix, samples
    async def _generate_single_sample(self, category: SampleCategory, asset_type: str, title: str) -> GeneratedSample:
        start_time = datetime.now()
        page_info = {
            'title': title,
            'category': category.name.lower().replace(' ', '_'),
            'asset_type': asset_type,
            'section': category.section_theme.value,
            'tier': category.visual_tier.value
        }
        competition = await self.orchestrator.generate_competitive_prompts(page_info)
        base_prompt = self.yaml_system._generate_enhanced_prompt(
            title, asset_type, category.section_theme.value
        )
        enhanced_prompts = [variant.prompt for variant in competition.variants]
        emotional_markers = []
        luxury_indicators = []
        for variant in competition.variants:
            emotional_markers.extend(variant.emotional_markers)
            luxury_indicators.extend(variant.luxury_indicators)
        emotional_markers = list(dict.fromkeys(emotional_markers))
        luxury_indicators = list(dict.fromkeys(luxury_indicators))
        generation_time = (datetime.now() - start_time).total_seconds()
        sample = GeneratedSample(
            category=category.name,
            asset_type=asset_type,
            title=title,
            base_prompt=base_prompt,
            enhanced_prompts=enhanced_prompts,
            emotional_markers=emotional_markers,
            luxury_indicators=luxury_indicators,
            visual_tier=category.visual_tier.value,
            section_theme=category.section_theme.value,
            generation_time=generation_time
        )
        return sample
    def save_sample_matrix(self, matrix: SampleMatrix, samples: List[GeneratedSample],
                          output_file: str = "sample_matrix_results.json") -> Path:
        output_path = Path(output_file)
        data = {
            'matrix_config': {
                'categories': [
                    {
                        'name': cat.name,
                        'visual_tier': cat.visual_tier.value,
                        'section_theme': cat.section_theme.value,
                        'emotional_context': cat.emotional_context.value,
                        'comfort_level': cat.comfort_level.value,
                        'sample_titles': cat.sample_titles
                    }
                    for cat in matrix.categories
                ],
                'asset_types': matrix.asset_types,
                'total_samples': matrix.total_samples,
                'generation_timestamp': matrix.generation_timestamp
            },
            'samples': [asdict(sample) for sample in samples],
            'summary': self._generate_summary(samples)
        }
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        self.logger.info(f"Sample matrix saved to {output_path}")
        return output_path
    def _generate_summary(self, samples: List[GeneratedSample]) -> Dict[str, Any]:
        total_samples = len(samples)
        category_counts = {}
        for sample in samples:
            category_counts[sample.category] = category_counts.get(sample.category, 0) + 1
        asset_type_counts = {}
        for sample in samples:
            asset_type_counts[sample.asset_type] = asset_type_counts.get(sample.asset_type, 0) + 1
        generation_times = [s.generation_time for s in samples if s.generation_time]
        avg_generation_time = sum(generation_times) / len(generation_times) if generation_times else 0
        all_emotional_markers = set()
        all_luxury_indicators = set()
        for sample in samples:
            all_emotional_markers.update(sample.emotional_markers)
            all_luxury_indicators.update(sample.luxury_indicators)
        return {
            'total_samples': total_samples,
            'category_distribution': category_counts,
            'asset_type_distribution': asset_type_counts,
            'average_generation_time_seconds': round(avg_generation_time, 2),
            'unique_emotional_markers': len(all_emotional_markers),
            'unique_luxury_indicators': len(all_luxury_indicators),
            'emotional_markers_list': sorted(list(all_emotional_markers)),
            'luxury_indicators_list': sorted(list(all_luxury_indicators))
        }
    def generate_quality_report(self, samples: List[GeneratedSample]) -> Dict[str, Any]:
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_samples_analyzed': len(samples),
            'quality_metrics': {}
        }
        emotional_contexts = set()
        comfort_levels = set()
        for sample in samples:
            emotional_marker_count = len(sample.emotional_markers)
            luxury_indicator_count = len(sample.luxury_indicators)
            emotional_contexts.add(sample.category)
        report['quality_metrics'] = {
            'emotional_marker_coverage': {
                'average_markers_per_sample': sum(len(s.emotional_markers) for s in samples) / len(samples),
                'samples_with_emotional_markers': sum(1 for s in samples if s.emotional_markers),
                'unique_emotional_contexts_covered': len(emotional_contexts)
            },
            'luxury_aesthetic_coverage': {
                'average_luxury_indicators_per_sample': sum(len(s.luxury_indicators) for s in samples) / len(samples),
                'samples_with_luxury_indicators': sum(1 for s in samples if s.luxury_indicators),
                'luxury_coverage_percentage': (sum(1 for s in samples if s.luxury_indicators) / len(samples)) * 100
            },
            'prompt_quality': {
                'average_enhanced_prompts_per_sample': sum(len(s.enhanced_prompts) for s in samples) / len(samples),
                'samples_with_multiple_model_variants': sum(1 for s in samples if len(s.enhanced_prompts) > 1)
            }
        }
        return report
async def test_sample_generator():
    generator = SampleGenerator()
    print("🎨 Generating 3x3 sample matrix for Estate Planning Concierge v4.0...")
    matrix, samples = await generator.generate_sample_matrix()
    output_file = generator.save_sample_matrix(matrix, samples)
    quality_report = generator.generate_quality_report(samples)
    quality_file = Path("sample_quality_report.json")
    with open(quality_file, 'w') as f:
        json.dump(quality_report, f, indent=2)
    print(f"\n✅ Sample generation complete!")
    print(f"📊 Generated {len(samples)} samples across 3x3 matrix")
    print(f"📁 Results saved to: {output_file}")
    print(f"📈 Quality report: {quality_file}")
    summary = generator._generate_summary(samples)
    print(f"\n📋 Summary:")
    print(f"  Categories: {len(summary['category_distribution'])}")
    print(f"  Asset types: {len(summary['asset_type_distribution'])}")
    print(f"  Avg generation time: {summary['average_generation_time_seconds']}s")
    print(f"  Emotional markers: {summary['unique_emotional_markers']}")
    print(f"  Luxury indicators: {summary['unique_luxury_indicators']}")
    return output_file, quality_file
if __name__ == "__main__":
    asyncio.run(test_sample_generator())

================================================================================
FILE: asset_generation/config.json
================================================================================

{
  "replicate": {
    "api_key": "${REPLICATE_API_KEY}",
    "rate_limit": 2,
    "timeout": 30,
    "retry": 3,
    "models": {
      "icons": {
        "model_id": "black-forest-labs/recraft-v3-svg",
        "cost_per_image": 0.04,
        "style": "realistic_image",
        "purpose": "Technical drawing icons with mechanical poetry aesthetic"
      },
      "covers": {
        "model_id": "black-forest-labs/flux-1.1-pro",
        "cost_per_image": 0.04,
        "style": "photorealistic",
        "purpose": "Blueprint-style technical covers with layered compositions"
      },
      "textures": {
        "model_id": "stability-ai/sdxl",
        "cost_per_image": 0.003,
        "style": "texture",
        "purpose": "Background textures and patterns"
      },
      "letter_headers": {
        "model_id": "black-forest-labs/flux-1.1-pro",
        "cost_per_image": 0.04,
        "style": "professional_letterhead",
        "purpose": "Formal letterhead designs for template letters"
      },
      "database_icons": {
        "model_id": "black-forest-labs/recraft-v3-svg",
        "cost_per_image": 0.04,
        "style": "data_visualization",
        "purpose": "Clean icons for database category organization"
      }
    }
  },
  "review": {
    "port": 4500,
    "auto_open": true,
    "host": "localhost",
    "approval_file": "APPROVED.txt",
    "production_approval_file": "PRODUCTION_APPROVED.txt"
  },
  "budget": {
    "sample_generation": {
      "max_cost": 1.00,
      "approval_required": true,
      "items": 10,
      "alert_threshold": 0.80
    },
    "mass_generation": {
      "max_cost": 25.00,
      "approval_required": true,
      "items": "dynamic",
      "alert_threshold": 20.00,
      "comment": "Dynamically discovers ~490 assets from YAML files"
    },
    "daily_limit": 25.00
  },
  "assets": {
    "icons": {
      "count": "dynamic",
      "format": "svg",
      "dimensions": "64x64",
      "style": "mechanical_poetry",
      "comment": "~230 icons discovered from YAML"
    },
    "covers": {
      "count": "dynamic",
      "format": "png",
      "dimensions": "1500x400",
      "style": "blueprint_technical",
      "comment": "~230 covers discovered from YAML"
    },
    "textures": {
      "count": 10,
      "format": "png",
      "dimensions": "512x512",
      "style": "seamless_pattern"
    },
    "letter_headers": {
      "count": 18,
      "format": "png",
      "dimensions": "1920x400",
      "style": "professional_letterhead"
    },
    "database_icons": {
      "count": 10,
      "format": "svg",
      "dimensions": "48x48",
      "style": "data_visualization"
    }
  },
  "theme": {
    "name": "Estate Planning Executive",
    "description": "Single high-end theme with mechanical poetry icons and blueprint covers",
    "colors": {
      "primary": "#4A7C74",
      "secondary": "#527B84",
      "accent": "#90CDF4",
      "background": "#F9FAFB",
      "text": "#1F2937"
    },
    "total_assets": "~490 (dynamically discovered)",
    "estimated_cost": 19.60
  },
  "logging": {
    "level": "INFO",
    "colored_output": true,
    "progress_bars": true,
    "log_file": "logs/asset_generation.log",
    "max_log_size": 10485760,
    "backup_count": 5
  },
  "output": {
    "sample_directory": "output/samples",
    "production_directory": "output/production",
    "backup_directory": "output/backup",
    "github_repo": "notion-estate-assets",
    "github_branch": "main"
  }
}

================================================================================
FILE: asset_generation/requirements.txt
================================================================================

# Estate Planning Concierge v4.0 Asset Generation Requirements
replicate>=0.25.0
colorama>=0.4.6
tqdm>=4.66.0
aiohttp>=3.9.0
Pillow>=10.0.0
python-dotenv>=1.0.0
requests>=2.31.0
PyYAML>=6.0.1

================================================================================
FILE: asset_generation/meta_prompts/master_prompt.txt
================================================================================

# ESTATE PLANNING CONCIERGE v4.0 - MASTER PROMPT FOR IMAGE GENERATION

You are an expert luxury branding consultant specializing in estate planning visual communications. Your task is to generate precise, high-quality image generation prompts for estate planning marketing materials.

## CONTEXT UNDERSTANDING

You will receive context data including:
- **Visual Tier**: The hierarchical level (HUB, SECTION, DOCUMENT, LETTER, DIGITAL)
- **Emotional Context**: The sensitivity level needed (LOSS_PROCESSING, CELEBRATION_PLANNING, etc.)
- **Asset Type**: The specific type of marketing material being created
- **Target Audience**: Professional estate planning clientele

## VISUAL HIERARCHY REQUIREMENTS

**TIER 1 - HUB**: Ultra-premium, commanding presence, sophisticated color palettes, executive-level imagery
**TIER 2 - SECTION**: Professional elegance, refined typography, trust-building visuals
**TIER 3 - DOCUMENT**: Clean, accessible, informative layouts with subtle luxury touches
**TIER 4 - LETTER**: Personal, warm, approachable while maintaining sophistication  
**TIER 5 - DIGITAL**: Modern, engaging, mobile-optimized with luxury aesthetic

## EMOTIONAL SENSITIVITY GUIDELINES

- **LOSS_PROCESSING**: Compassionate, gentle, respectful imagery with soft lighting
- **CELEBRATION_PLANNING**: Warm, hopeful, family-focused with elegant presentation
- **HEALTH_CONCERNS**: Caring, professional, reassuring with medical sensitivity
- **FAMILY_HARMONY**: Unifying, peaceful, inclusive imagery promoting togetherness
- **WEALTH_TRANSITION**: Sophisticated, generational, legacy-focused visuals
- **CHARITABLE_GIVING**: Inspiring, community-minded, philanthropic themes
- **DIGNIFIED_PLANNING**: Respectful, thorough, professional estate planning imagery

## LUXURY AESTHETIC STANDARDS

- Premium color palettes: Deep blues, elegant golds, sophisticated grays
- High-quality photography with professional lighting
- Refined typography and clean layouts
- Subtle textures and premium materials
- Executive-level presentation quality
- Timeless, not trendy design elements

## REQUIRED OUTPUT FORMAT

You MUST respond with the following structured format exactly:

---
SYSTEM: [Your system message for the image generation API - include all technical parameters, style requirements, and quality specifications]

TEMPERATURE: [Optimal temperature value between 0.1-1.0 for consistent, high-quality results]

ROLE: [The specific role/persona for the image generation - e.g., "luxury brand designer", "estate planning visual consultant"]

PROMPT: [The complete, detailed image generation prompt with specific visual elements, composition, lighting, colors, mood, and technical specifications]
---

## CRITICAL REQUIREMENTS

1. **System Message**: Must include all technical parameters needed for premium image generation
2. **Temperature**: Choose optimal value for consistency (typically 0.3-0.7 for professional work)
3. **Role**: Define clear expertise area relevant to estate planning and luxury branding
4. **Prompt**: Comprehensive, specific, actionable for high-quality image generation

## QUALITY STANDARDS

Your generated prompts must produce:
- Professional-grade imagery suitable for high-net-worth clients
- Consistent brand aesthetic across all materials
- Emotionally appropriate content for estate planning context
- Luxury presentation quality meeting executive expectations
- Clear, specific visual direction for reliable results

Remember: You are creating visual communications for sensitive family matters involving significant wealth and legacy decisions. Every image must reflect the gravity, sophistication, and trustworthiness required for this field.

Generate your structured response now based on the provided context data.

================================================================================
FILE: split_yaml/09_admin_rollout_setup.yaml
================================================================================

# 09_admin_rollout_setup.yaml
complexity: moderate
# Admin-only rollout helpers. Delete this branch before sharing.
pages:
  - title: "Admin – Rollout"
    icon: "emoji:🧩"
    description: "Admin-only setup workspace. Complete these steps, verify Rollout Summary hits 100%, then delete this branch."
    hub: false
    children:

      - title: "Admin – Rollup Setup Guide"
        icon: "emoji:📈"
        description: "Configure UI rollups in Notion (manual, fast). These unlock live totals in Estate Analytics and Hub summaries."
        body:
          - type: callout
            icon: "emoji:⚠️"
            color: gray_background
            text: "ADMIN_ONLY • ROLLUP_GUIDE_MARKER • These steps are visible only to the admin. Delete the Admin – Rollout branch before sharing."
          - type: toggle
            summary: "Accounts → Estate Analytics (Liquid Assets)"
            children:
              - type: paragraph
                text: >
                  1) Open “Estate Analytics” DB → add a **Rollup** property named **UI: Liquid Assets**.
                  2) Relation: **Related Page** → Pages Index → filter to rows for **Account** pages.
                  3) Rollup property on target: **Balance** (or your chosen balance field).
                  4) Function: **Sum**.
                  5) Confirm the **Total Liquid Assets** formula prefers this UI rollup.
          - type: toggle
            summary: "Properties → Estate Analytics (Real Property Value)"
            children:
              - type: paragraph
                text: >
                  Add **UI: Real Property Value** rollup → target **Estimated Value** from **Property** pages (via Pages Index). Function: **Sum**.
          - type: toggle
            summary: "Insurance → Estate Analytics (Coverage)"
            children:
              - type: paragraph
                text: >
                  Add **UI: Insurance Coverage** rollup → target **Coverage Amount** from **Insurance** pages (via Pages Index). Function: **Sum**.
          - type: paragraph
            text: "When all three are done, remove this callout or type DONE here: [ADMIN ROLLUP DONE]"

      - title: "Admin – Views Setup Guide"
        icon: "emoji:🗂️"
        description: "Create saved views so users see clean, focused lists by default."
        body:
          - type: callout
            icon: "emoji:⚠️"
            color: gray_background
            text: "ADMIN_ONLY • VIEWS_GUIDE_MARKER • Create these **Saved Views** and set as default."
          - type: toggle
            summary: "Accounts – Active only"
            children:
              - type: paragraph
                text: >
                  Open **Accounts** DB → add a view named **Active** with filter **Archive Flag != Archive**.
                  Set as **Default**.
          - type: toggle
            summary: "Transactions – By account"
            children:
              - type: paragraph
                text: >
                  In **Transactions**, add a view **By Account** grouped by **Related Page**.
                  Add a filter for a specific account for screenshots/demos.
          - type: toggle
            summary: "Properties – Active only"
            children:
              - type: paragraph
                text: >
                  In **Properties**, add view **Active** with filter **Archive Flag != Archive**.
          - type: toggle
            summary: "Insurance Claims – Open/Submitted only"
            children:
              - type: paragraph
                text: >
                  In **Insurance Claims**, add view **Open** with filter **Status is Open/Submitted/In Review**.
          - type: paragraph
            text: "When all saved views are created and defaults set, type DONE here: [ADMIN VIEWS DONE]"

      - title: "Admin – QA Checklist"
        icon: "emoji:✅"
        description: "Final checks before deleting the Admin branch."
        body:
          - type: callout
            icon: "emoji:⚠️"
            color: gray_background
            text: "ADMIN_ONLY • QA_MARKER • Confirm each item, then delete this Admin branch."
          - type: bulleted_list
            items:
              - "Rollout Summary shows 100%"
              - "UI rollups (Liquid/Property/Insurance) present in **Estate Analytics**"
              - "Active-only views set as default on major DBs"
              - "Hubs show progress snapshots and embedded views"
              - "Letters and Legal pages reviewed for tone and placeholders removed"

acceptance:
  rows:
    - Title: "Set UI rollup: UI: Liquid Assets → Accounts"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Set UI rollup: UI: Real Property Value → Properties"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Set UI rollup: UI: Insurance Coverage → Insurance"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Create Active-only views (Accounts/Properties)"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Create Open claims view (Insurance Claims)"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Verify Hubs show progress + embeds"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"
    - Title: "Delete Admin – Rollout branch"
      Section: "Admin"
      Status: "Not Started"
      Role: "Owner"


